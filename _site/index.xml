<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>All Your Bayes</title>
<link>https://allyourbayes.com/</link>
<atom:link href="https://allyourbayes.com/index.xml" rel="self" type="application/rss+xml"/>
<description>thoughts on risk and uncertainty in decision support</description>
<generator>quarto-1.7.30</generator>
<lastBuildDate>Thu, 25 Jan 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>The Turing Podcast</title>
  <dc:creator>Domenic Di Francesco</dc:creator>
  <link>https://allyourbayes.com/posts/Turing_Podcast/</link>
  <description><![CDATA[ 






<section id="tldr" class="level3">
<h3 class="anchored" data-anchor-id="tldr">TLDR</h3>
<p>Last year, I participated in an episode of the Alan Turing Institute’s Coffee Pod podcast. It was hosted by my friend <a href="https://twitter.com/mooniean">Bea</a> and we talked about my background in engineering, my PhD and transition into computational statistics and machine learning, my interest in football, and my (then) new puppy Ada.</p>
<p>Here’s the episode, I hope you enjoy it!</p>
<hr>
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/episode/5FqU7ZRATxp9CIwHvD7pvP?utm_source=generator&amp;theme=0&amp;t=0" width="100%" height="352" frameborder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy">
</iframe>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{di_francesco2024,
  author = {Di Francesco, Domenic},
  title = {The {Turing} {Podcast}},
  date = {2024-01-25},
  url = {https://allyourbayes.com/posts/Turing_Podcast/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-di_francesco2024" class="csl-entry quarto-appendix-citeas">
Di Francesco, Domenic. 2024. <span>“The Turing Podcast.”</span> January
25, 2024. <a href="https://allyourbayes.com/posts/Turing_Podcast/">https://allyourbayes.com/posts/Turing_Podcast/</a>.
</div></div></section></div> ]]></description>
  <category>Podcast</category>
  <category>Bayes</category>
  <category>Engineering</category>
  <category>Dogs</category>
  <guid>https://allyourbayes.com/posts/Turing_Podcast/</guid>
  <pubDate>Thu, 25 Jan 2024 00:00:00 GMT</pubDate>
  <media:content url="https://allyourbayes.com/posts/Turing_Podcast/alan_turing.png" medium="image" type="image/png" height="104" width="144"/>
</item>
<item>
  <title>Who Am I?</title>
  <dc:creator>Domenic Di Francesco</dc:creator>
  <link>https://allyourbayes.com/posts/Who_Am_I/</link>
  <description><![CDATA[ 






<section id="tldr" class="level3">
<h3 class="anchored" data-anchor-id="tldr">TLDR</h3>
<p>At a previous job, my colleagues and I would occasionally create football (soccer) quizzes for each other. The game I used to send them would involve me sequentially sending a list of (increasingly helpful) clues, from which they had to guess which player I was thinking of. I have now turned this into a web app using the <a href="https://shiny.rstudio.com">shiny</a> and <a href="https://deanattali.com/shinyjs/">shinyjs</a> <code>R</code> libraries, and you can play it here. You can find the code on the GitHub link that I have included in the app.</p>
<p>I hope you enjoy it!</p>
<p>Edit: I will try and update this regularly with new players &amp; clues, so keep checking back. Suggestions are welcome - you can <a href="https://twitter.com/Domenic_DF">message me on Twitter</a>.</p>
<hr>
<iframe height="750" width="100%" frameborder="no" src="https://0197c161-3395-97fd-ba22-c8e137c72f5c.share.connect.posit.cloud/">
</iframe>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{di_francesco2023,
  author = {Di Francesco, Domenic},
  title = {Who {Am} {I?}},
  date = {2023-03-21},
  url = {https://allyourbayes.com/posts/Who_Am_I/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-di_francesco2023" class="csl-entry quarto-appendix-citeas">
Di Francesco, Domenic. 2023. <span>“Who Am I?”</span> March 21, 2023. <a href="https://allyourbayes.com/posts/Who_Am_I/">https://allyourbayes.com/posts/Who_Am_I/</a>.
</div></div></section></div> ]]></description>
  <category>Football</category>
  <category>R</category>
  <category>Shiny</category>
  <guid>https://allyourbayes.com/posts/Who_Am_I/</guid>
  <pubDate>Tue, 21 Mar 2023 00:00:00 GMT</pubDate>
  <media:content url="https://allyourbayes.com/posts/Who_Am_I/game.png" medium="image" type="image/png" height="77" width="144"/>
</item>
<item>
  <title>Bayes@Lund2023</title>
  <dc:creator>Domenic Di Francesco</dc:creator>
  <link>https://allyourbayes.com/posts/Bayes@Lund/</link>
  <description><![CDATA[ 






<section id="tldr" class="level3">
<h3 class="anchored" data-anchor-id="tldr">TLDR</h3>
<p>A recording of my presentation on value of information analysis at the Bayes@Lund2023 conference.</p>
<hr>
<section id="bayeslund2023-conference" class="level4">
<h4 class="anchored" data-anchor-id="bayeslund2023-conference">Bayes@Lund2023 Conference</h4>
<p>I have been following the Bayes@Lund conference since I started my PhD, and have often found the work presented to be very useful. This year I was able to attend and I presented on the topic of <em>value of information analysis</em> (how much should we be willing to pay for data).</p>
<p>Conventional experimental design is used to identify where our next mesaurement(s) should be obtained on the bases of reducing uncertainty. However, this scale (some measure of information entropy) is not always intuitive, and it won’t tell you the point at which paying for another measurement becomes uneconomical.</p>
<p>Value of information analysis is used to quantify how much we should be willing to pay for data of a specified quality (precision, bias, reliability, completeness, etc.), in the context of helping us make decisions.</p>
<p>Below is the recording of my talk, which breifly introduces the topic and provides a couple of examples.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/zJgUnjJjFrk" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{di_francesco2023,
  author = {Di Francesco, Domenic},
  title = {Bayes@Lund2023},
  date = {2023-01-23},
  url = {https://allyourbayes.com/posts/Bayes@Lund/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-di_francesco2023" class="csl-entry quarto-appendix-citeas">
Di Francesco, Domenic. 2023. <span>“Bayes@Lund2023.”</span> January 23,
2023. <a href="https://allyourbayes.com/posts/Bayes@Lund/">https://allyourbayes.com/posts/Bayes@Lund/</a>.
</div></div></section></div> ]]></description>
  <category>Bayes</category>
  <category>Stan</category>
  <category>Julia</category>
  <category>Decisions</category>
  <category>Value of Information</category>
  <guid>https://allyourbayes.com/posts/Bayes@Lund/</guid>
  <pubDate>Mon, 23 Jan 2023 00:00:00 GMT</pubDate>
  <media:content url="https://allyourbayes.com/posts/Bayes@Lund/VoPI.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>All Your Bayes 2.0</title>
  <link>https://allyourbayes.com/posts/welcome/</link>
  <description><![CDATA[ 






<p>I’ve recreated this site in <a href="https://quarto.org">quarto</a>, and plan on writing more jargon-free articles on risk, uncertainty, decisions, statistics, and football.</p>



 ]]></description>
  <category>news</category>
  <guid>https://allyourbayes.com/posts/welcome/</guid>
  <pubDate>Mon, 23 Jan 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Uncertainty in xG. Part 2: Partial Pooling</title>
  <dc:creator>Domenic Di Francesco</dc:creator>
  <link>https://allyourbayes.com/posts/xg_pt2/</link>
  <description><![CDATA[ 






<section id="tldr" class="level3">
<h3 class="anchored" data-anchor-id="tldr">TLDR</h3>
<p>This is part 2 of an article on fitting a Bayesian partial pooling model to predict expected goals. It has the benefits of (a) quantifying <em>aleatory and epistemic</em> uncertainty, and (b) making both group-level (player-specific) and population-level (team-specific) probabilistic predictions. If you are interested in these ideas but not in statistical language, then you can also check out <a href="https://allyourbayes.com/posts/xg_pt1/">part 1</a>.</p>
<hr>
</section>
<section id="expected-goals" class="level3">
<h3 class="anchored" data-anchor-id="expected-goals">Expected Goals</h3>
<p>Expected Goals (or <em>xG</em>) is a metric that was developed to predict the probability of a football (soccer) player scoring a goal, conditional on some mathematical characterisation of the shooting opportunity. Since we have a binary outcome (he or she will either score or not score) we can use everyone’s favourite GLM - logistic regression.</p>
<p>Unfortunately this causes some overlap with a <a href="https://www.allyourbayes.com/post/2020-02-14-bayesian-logistic-regression-with-stan/">previous blog post - ‘<em>Bayesian Logistic Regression with Stan</em>’</a>, but don’t worry - the focus here is all about <em>Partial Pooling</em>.</p>
<p>First let’s look at a non-Bayesian base case. <a href="https://statsbomb.com/">StatsBomb</a> have kindly made lots of football data freely available in <a href="https://github.com/statsbomb/StatsBombR">their R package</a>. The below creates a dataframe of the shots taken by Arsenal FC during the <code>2003</code>-<code>04</code> Premier League winning season.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(StatsBombR); <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(tidyverse)</span>
<span id="cb1-2"></span>
<span id="cb1-3">Prem_SB_matches <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">FreeMatches</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Competitions =</span> SB_comps <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb1-4">                               dplyr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(competition_name <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Premier League'</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb1-5">                               dplyr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(competition_gender <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'male'</span>))</span>
<span id="cb1-6"></span>
<span id="cb1-7">Arsenal_0304_shots <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">StatsBombFreeEvents</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">MatchesDF =</span> Prem_SB_matches, </span>
<span id="cb1-8">                                          <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Parallel =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb1-9">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">allclean</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb1-10">  dplyr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(type.name <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Shot'</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb1-11">  dplyr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(possession_team.name <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Arsenal'</span>)</span></code></pre></div>
</div>
<p>Using <code>R</code>’s <code>tidymodels</code> framework - make sure to have a look at <a href="https://www.youtube.com/channel/UCTTBgWyJl2HrrhQOOc710kA">Julia Silge’s tutorials</a> if you are unfamiliar - we can specify and fit a logistic regression. The below compares our results (including confidence intervals) to those from StatsBomb.</p>
<p>If you are interested in creating something similar yourself, this model has standardised inputs for parameters with relatively large values (such as angles and distances) and one hot encoding of categorical inputs (such as whether or not the shot was taken with a players weaker foot).</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/xg_pt2/index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Since we have used StatsBomb data (though their model will no doubt be based on a much larger collection) we would expect our results to be similar to theirs, and they are. Considering just the point estimates, the two models appear to broadly agree, especially when both are predicting a very low or a very high xG.</p>
<p>However, some of the confidence intervals on our <code>tidymodels</code> predictions are very large. Although we would generally expect these to decrease as we introduced more data, we know that football matches (and especially specific events within football matches) are full of uncertainty. If we want to be able to quantify this uncertainty in a more useful way (we do) - we want a Bayesian model. The below section details the specific type of Bayesian model that I’m proposing for estimating xG.</p>
</section>
<section id="multi-level-partial-pooling-models" class="level3">
<h3 class="anchored" data-anchor-id="multi-level-partial-pooling-models">Multi-Level (Partial Pooling) Models</h3>
<p>Hierarchical (or ‘nested’) data contains multiple groups within a population, such as players with a football team. Unfortunately, this information is lost (and bias is introduced) when such data is modelled as a single population. At the other extreme we can assume each group is fully independent, and the difficulty here is that there will be less data available and therefore more variance in our predictions.</p>
<p>Consequently, we want an intermediate solution, acknowledging variation between groups, but allowing for data from one group to inform predictions about others. This is achieved by using a multi-level (or hierarchical) model structure. Such models allow partial sharing (or <em>pooling</em>) of information between groups, to the extent that the data indicate is appropriate. This approach results in reduced variance (when compared to a set of corresponding independent models), a shift towards a population mean (known as <em>shrinkage</em>), and generally an improved predictive performance.</p>
<p>Sounds great, right? So why would anyone ever not use this kind of model? In his <a href="https://elevanth.org/blog/2017/08/24/multilevel-regression-as-default/">excellent blog</a>, Richard McElreath makes the case that multi-level models should be our default approach. His greatest criticism of them is that they require some experience or training to specify and interpret. <a href="https://xcelab.net/rm/statistical-rethinking/">His book</a> has a dedicated chapter to help with that. Of course, there are many better descriptions of multi-level modelling than you will get from me, but I personally found the examples in <a href="http://www.stat.columbia.edu/~gelman/arm/">Andrew Gelman and Jennifer Hill’s book</a> to be very helpful. Finally, <a href="https://betanalpha.github.io/assets/case_studies/hierarchical_modeling.html">Michael Betancourt has written a much more comprehensive blog post on the topic</a>, which includes a discussion on the underlying assumption of <em>exchangeability</em>.</p>
<p>We can create a partial pooling model by re-writing the below:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AxG%20=%20Inverse%20%5C;%20Logit(%5Calpha%20+%20%5Cbeta%20%5Ccdot%20X)%0A"></p>
<p>To look like this:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AxG%20=%20Inverse%20%5C;%20Logit(%5Calpha_%7B%5BPlayer%5D%7D%20+%20%5Cbeta_%7B%5BPlayer%5D%7D%20%5Ccdot%20X)%0A"></p>
<p>In this new structure, each parameter will now be a vector of length <img src="https://latex.codecogs.com/png.latex?N"> (where <img src="https://latex.codecogs.com/png.latex?N"> players are being considered). This means there will be a different co-efficient describing how <img src="https://latex.codecogs.com/png.latex?xG"> varies with distance from goal for each player. This makes sense as we would expect variation between players and we want our model to be able to describe it.</p>
<p>If each of these parameters had their own priors, we would essentially have specified <img src="https://latex.codecogs.com/png.latex?N"> independent models - one for each player. But there is a twist here: each of the vectors of co-efficients share a single prior.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbeta%20%5Csim%20N(%5Cmu_%7B%5Cbeta%7D,%20%5C;%20%5Csigma_%7B%5Cbeta%7D)%0A"></p>
<p>This will pull each of the individual co-efficients towards a shared mean, <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7B%5Cbeta%7D">. The variation between the players (for a given parameter) is characterised by <img src="https://latex.codecogs.com/png.latex?%5Csigma_%7B%5Cbeta%7D">. Rather than specify these ourselves, we will also estimate these as part of the model. This means that the extent of the pooling is conditional on the data, which is an extremely useful feature. However, we then need to include priors on these parameters, which are known as <em>hyperpriors</em>.</p>
<p>Note that this process has introduced an extra layer (or level) to the model structure. This is why they are known as <em>multi-level</em> or <em>hierarchical</em> models. The term <em>partial pooling</em> is more a description of what they do.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/nXvirfLCf99rG/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>In the Absence of Multi-Level Models</figcaption>
</figure>
</div>
<p>We see the greatest benefit of this approach when only limited data is available for one or more groups. If one player took very few shots during a period of data collection, then there will be a lot of uncertainty in their xG predictions ….<em>unless</em> we can make use of the data we have for the rest of the team.</p>
</section>
<section id="what-does-this-look-like-in-stan" class="level3">
<h3 class="anchored" data-anchor-id="what-does-this-look-like-in-stan">What does this look like in <code>Stan</code>?</h3>
<p>The below is a reduced <code>Stan</code> model, with just one co-efficient (concerning the distance from goal of the shot). This is not me being secretive, its just that the full model is quite large. You can simply add more parameters like a multi-variate linear regression on the log-odds scale, but remember that they will each require priors, hyperpriors, and data.</p>
<div class="cell" data-output.var="xG_model_reduced">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode stan code-with-copy"><code class="sourceCode stan"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">data</span> {</span>
<span id="cb2-2"></span>
<span id="cb2-3">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> &lt;<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lower</span> = <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>&gt; n_shots;</span>
<span id="cb2-4">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> &lt;<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lower</span> = <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">upper</span> = <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>&gt; goal [n_shots];</span>
<span id="cb2-5">  </span>
<span id="cb2-6">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> &lt;<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lower</span> = <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>&gt; n_players;</span>
<span id="cb2-7">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> &lt;<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lower</span> = <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>&gt; player_id [n_shots];</span>
<span id="cb2-8">  </span>
<span id="cb2-9">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">vector</span> [n_shots] dist_goal;</span>
<span id="cb2-10"></span>
<span id="cb2-11">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">real</span> mu_mu_alpha;</span>
<span id="cb2-12">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">real</span> &lt;<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lower</span> = <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>&gt; sigma_mu_alpha;</span>
<span id="cb2-13">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">real</span>&lt;<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lower</span> = <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>&gt; rate_sigma_alpha;</span>
<span id="cb2-14">  </span>
<span id="cb2-15">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">real</span> mu_mu_beta_dist_goal;</span>
<span id="cb2-16">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">real</span> &lt;<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lower</span> = <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>&gt; sigma_mu_beta_dist_goal;</span>
<span id="cb2-17">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">real</span>&lt;<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lower</span> = <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>&gt; rate_sigma_beta_dist_goal;</span>
<span id="cb2-18">  </span>
<span id="cb2-19">}</span>
<span id="cb2-20"></span>
<span id="cb2-21"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">parameters</span> {</span>
<span id="cb2-22">  </span>
<span id="cb2-23">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">vector</span> [n_players] alpha;</span>
<span id="cb2-24">  </span>
<span id="cb2-25">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">vector</span> [n_players] beta_dist_goal;</span>
<span id="cb2-26"></span>
<span id="cb2-27">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">real</span> mu_alpha;</span>
<span id="cb2-28">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">real</span> &lt;<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lower</span> = <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>&gt; sigma_alpha;</span>
<span id="cb2-29">  </span>
<span id="cb2-30">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">real</span> mu_beta_dist_goal;</span>
<span id="cb2-31">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">real</span> &lt;<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lower</span> = <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>&gt; sigma_beta_dist_goal;</span>
<span id="cb2-32">  </span>
<span id="cb2-33">}</span>
<span id="cb2-34"></span>
<span id="cb2-35"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">model</span> {</span>
<span id="cb2-36"></span>
<span id="cb2-37">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Logistic model </span></span>
<span id="cb2-38">  </span>
<span id="cb2-39">  goal ~ bernoulli_logit(alpha[player_id] + beta_dist_goal[player_id] .* dist_goal); </span>
<span id="cb2-40"></span>
<span id="cb2-41">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Priors </span></span>
<span id="cb2-42">  </span>
<span id="cb2-43">  alpha ~ normal(mu_alpha, sigma_alpha);</span>
<span id="cb2-44">  beta_dist_goal ~ normal(mu_beta_dist_goal, sigma_beta_dist_goal);</span>
<span id="cb2-45"></span>
<span id="cb2-46">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Hyperpriors</span></span>
<span id="cb2-47">  </span>
<span id="cb2-48">  mu_alpha ~ normal(mu_mu_alpha, sigma_mu_alpha);</span>
<span id="cb2-49">  sigma_alpha ~ exponential(rate_sigma_alpha);</span>
<span id="cb2-50">  </span>
<span id="cb2-51">  mu_beta_dist_goal ~ normal(mu_mu_beta_dist_goal, sigma_mu_beta_dist_goal);</span>
<span id="cb2-52">  sigma_beta_dist_goal ~ exponential(rate_sigma_beta_dist_goal);</span>
<span id="cb2-53">  </span>
<span id="cb2-54">}</span>
<span id="cb2-55"></span>
<span id="cb2-56"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">generated quantities</span> {</span>
<span id="cb2-57">  </span>
<span id="cb2-58">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">real</span> alpha_pp = normal_rng(mu_alpha, sigma_alpha);</span>
<span id="cb2-59">  <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">real</span> beta_dist_goal_pp = normal_rng(mu_beta_dist_goal, sigma_beta_dist_goal);</span>
<span id="cb2-60"></span>
<span id="cb2-61">}</span></code></pre></div>
</div>
<p>A few things that I’d like to note:</p>
<ul>
<li>My input data is of length <code>n_shots</code> and my parameters are vectors of length <code>n_players</code>.</li>
<li>I’ve included my hyperpriors (the <code>mu_mu_...</code>, <code>sigma_mu...</code>, and <code>rate_sigma...</code> terms) as data, rather than <em>hard code</em> values into the file. This is so I can re-run the model with new hyperpriors without <code>Stan</code> needing to recompile.</li>
<li>Even though I have included the <code>mu...</code> and <code>sigma..</code> terms as priors in my comment, this is just to help describe the model structure. They are all included in the Parameters block of the model. As discussed above, they are inferred as part of the joint posterior distribution, meaning that we are estimating the extent of the pooling from the data.</li>
<li>I’m using the generated quantities to produce my population-level parameters, so that I have everything I need to put together probabilistic predictions in either <code>R</code> or <code>Python</code>.</li>
</ul>
<p>#### Model Parameters</p>
<p>The posterior distribution (which <code>Stan</code> has sampled from) is a joint probabilistic model of all parameters. Let’s have a look at a few, specifically those corresponding to the effect of distance between the shot taker and goalkeeper. Shown below is the co-efficient for <img src="https://latex.codecogs.com/png.latex?6"> players (indexed <img src="https://latex.codecogs.com/png.latex?1%20%5Crightarrow%206">). We can see that the distance to the keeper is predicted to influence each player differently.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/xg_pt2/index_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Some of the players will have taken fewer shots and therefore we will have less data to fit their player-specific parameters. The <code>mu_beta_dist_keeper</code> and <code>sigma_beta_dist_keeper</code> parameters in the above plot are the shared ‘<em>priors</em>’ that control how the data from each of the players can be used to inform one another. The <code>beta_dist_keeper_pp</code> parameter is specified in the generated quantities block of my <code>Stan</code> model. It is correlated samples from the distribution characterised by the shared priors. This becomes the population (team) level co-efficient in my predictions.</p>
<p>I’ve included some predictions for some actual shots taken that season in <a href="https://allyourbayes.com/posts/xg_pt1/">part 1</a> of this article, but since this is the purpose of the model let’s look at one more.</p>
<p>Here is Robert Pirès goal from just outside the box at home to Bolton Wanderers in 2004. It was on his stronger (right) foot and he was not under pressure from any defenders.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/xg_pt2/index_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>As labelled on the above plot, the StatsBomb model only gave Pirès a 5% chance of scoring this chance. The below xG predictions are from the Bayesian partial pooling model, both for Robert Pirès (upper) and for the case where any Arsenal player could be shooting (lower). Also shown is the StatsBomb prediction. We see an improvement (since we know this chance was scored) when we make a player-specific prediction.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/xg_pt2/index_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Our probabilistic predictions contain more information than point estimates, but for the purposes of a simpler comparison we can consider the mean value. The mean value of our team-level prediction is 20%, but conditional on the knowledge that Pirès was shooting, this becomes 33%.</p>
<p>If Arsène Wenger could’ve chosen which of his players was presented with this opportunity, Robert Pirès would’ve been one of his top choices (though possible behind Thierry Henry). We have an intuitive understanding that such players have the necessary attributes to score from relatively difficult opportunities such as this, and this is accounted for in our model. We have tackled the challenge of greatly reduced (player-specific) datasets, by allowing them to share information on the basis of how similar they are.</p>
</section>
<section id="challenges" class="level3">
<h3 class="anchored" data-anchor-id="challenges">Challenges</h3>
<p>Multi-level models capture the multi-level structure of hierarchical (nested) datasets, accounting for both variability and commonality between different groups (in this example: between different players in a team). However, as we can see from the previous plot, by introducing a set of parameters for each group and relating them all in this way, the posterior distribution now has many more dimensions and is more challenging to sample from. If you are using <code>Stan</code> you may now see more warning messages regarding <em>divergent transitions</em> - a concept that José Mourinho is acting out, below. If you do run into these problems, I would recommend reviewing the <a href="https://mc-stan.org/docs/2_25/stan-users-guide/reparameterization-section.html">guidance in the Stan manual on reparameterisation</a> (writing your same model on a new scale, such that it is easier for the software to work with).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/140EFtM0NCyjHq/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>Mou’s Divergent Transitions</figcaption>
</figure>
</div>
<p>Finally, I have published a paper demonstrating this modelling approach in an engineering context, which includes additional details for anyone who is interested: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0951832020306189?via=ihub">‘Consistent and coherent treatment of uncertainties and dependencies in fatigue crack growth calculations using multi-level Bayesian models’</a>.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{di_francesco2021,
  author = {Di Francesco, Domenic},
  title = {Uncertainty in {xG.} {Part} 2},
  date = {2021-01-07},
  url = {https://allyourbayes.com/posts/xg_pt2/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-di_francesco2021" class="csl-entry quarto-appendix-citeas">
Di Francesco, Domenic. 2021. <span>“Uncertainty in xG. Part 2.”</span>
January 7, 2021. <a href="https://allyourbayes.com/posts/xg_pt2/">https://allyourbayes.com/posts/xg_pt2/</a>.
</div></div></section></div> ]]></description>
  <category>football</category>
  <category>analysis</category>
  <category>xg</category>
  <category>Stan</category>
  <category>uncertainty</category>
  <category>Bayes</category>
  <category>multi-level modelling</category>
  <category>partial pooling</category>
  <guid>https://allyourbayes.com/posts/xg_pt2/</guid>
  <pubDate>Thu, 07 Jan 2021 00:00:00 GMT</pubDate>
  <media:content url="https://allyourbayes.com/posts/xg_pt2/xg2.png" medium="image" type="image/png" height="104" width="144"/>
</item>
<item>
  <title>Uncertainty in xG. Part 1: Overview</title>
  <dc:creator>Domenic Di Francesco</dc:creator>
  <link>https://allyourbayes.com/posts/xg_pt1/</link>
  <description><![CDATA[ 






<section id="tldr" class="level3">
<h3 class="anchored" data-anchor-id="tldr">TLDR</h3>
<p>The Expected Goals (xG) metric is now widely recognised as numerical measure of the <em>quality</em> of a goal scoring opportunity in a football (soccer) match. In this article we consider how to deal with uncertainty in predicting xG, and how each players individual abilities can be accounted for. This is part 1 of the article, which is intended to be free of stats jargon, maths and code. If you are interested in those details, you can also check out <a href="https://allyourbayes.com/posts/xg_pt2/">part 2</a>.</p>
<hr>
</section>
<section id="what-are-expected-goals" class="level3">
<h3 class="anchored" data-anchor-id="what-are-expected-goals">What are Expected Goals?</h3>
<p><a href="https://www.optasports.com/services/analytics/advanced-metrics/#:~:text=Expected%20goals%20(xG)%20measures%20the,defined%20as%20a%20big%20chance.">Opta sports</a> tell us that the <em>Expected Goals</em> (or <strong>xG</strong>) of a shot describe how likely it is to be scored. The cumulative xG over a game will therefore give an indication of how many goals a team would usually score, based on the chances they created.</p>
<p>Why would anyone be interested in this? Because if the xG model is any good, it can be the basis for an evidence-based style of play. If certain individuals in a team enjoy shooting from long-distance (or any other set of circumstances associated with a low xG), they may be encouraged to keep possession until a more favourable (higher xG) chance arises.</p>
<p>There is no universally accepted way of calculating xG, so there are many competing models around. In this article I will describe a statistical model that cares about who is taking the shot, but does not treat each player as a separate independent case. More on this later…</p>
</section>
<section id="data-arsenals-invincibles-courtesy-of-statsbomb" class="level3">
<h3 class="anchored" data-anchor-id="data-arsenals-invincibles-courtesy-of-statsbomb">Data: Arsenal’s Invincibles (Courtesy of StatsBomb)</h3>
<p>Once upon a time (in the <code>2003</code>-<code>04</code> season), Arsenal FC were brilliant. That squad is still referred to as <em>the Invincibles</em> after finishing the season without a defeat in the league, scoring the most goals and conceding the fewest. Their top scorer, <a href="https://en.wikipedia.org/wiki/Thierry_Henry">Thierry Henry</a>, finished 4th in the Ballon d’Or voting this season (having finished 2nd the season before). Unfortunately José Mourinho arrived at Chelsea the following season and Arsenal haven’t won the league since.</p>
<p>I’m using Arsenal’s unbeaten league season as an example because <a href="https://statsbomb.com/">StatsBomb</a> have kindly made all this data freely available in <a href="https://github.com/statsbomb/StatsBombR">their R package</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/uBxP06JlaB7VCIaYXm/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>The Invincibles</figcaption>
</figure>
</div>
<p>Here are their league goal scorers:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/xg_pt1/index_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>And here’s where the goals were scored from:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/xg_pt1/index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The above plot shows that many of these goals were scored, even though the (StatsBomb) xG was relatively low. In fact the mean xG of the shots they scored was 0.33. This isn’t necessarily a problem as we do see improbable goals. Below is Giorgian De Arrascaeta’s contender for the 2020 FIFA Puskas award. Was anyone expecting him to score this chance? Could he do it again?</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/3icTC--cHfA" title="Arrascaeta's nominated goal for the 2020 FIFA Puskas award" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>An ideal xG model would correctly predict every goal without error, but the many sources of variability in the game means that this isn’t happening any time soon. A <strong>Bayesian</strong> model (such as the one I’m proposing) will include uncertainty in it’s predictions, letting us know when it can narrow down a predicted xG, and when there is a larger range of credible values based on the available information.</p>
</section>
<section id="would-henry-have-scored-it" class="level3">
<h3 class="anchored" data-anchor-id="would-henry-have-scored-it">Would Henry have scored it?</h3>
<p>Another feature that I’ve introduced to the model is the relationship between the data from different players. I want the model to distinguish between whether a team creates an opportunity for their top scorer, or their full-back who has never scored. One is clearly preferable, and should have a higher xG to reflect this. Why would this matter? Shooting from wide positions may (on average) be unlikely to pay off, but if your team has a winger who is especially adept at it, then it may be a strategy they should pursue.</p>
<p>For instance, Giorgian De Arrascaeta may have had a higher chance of scoring that bicycle kick when you consider that he was also nominated for the 2018 FIFA Puskas award for scoring another acrobatic volley.</p>
<p>The practical issue with considering each player separately is you now have many, smaller datasets. Larger datasets contain more information allowing for model parameters to be estimated more precisely. This sometimes encourages us to throw all our data into a single population and pretend we have a larger dataset. Your software will be happy, since it won’t know the difference, but you will lose the valuable player-specific information.</p>
<p>Bayesian models can do even better than this though. Consider some data that was collected from Arsenal’s defensive midfielder, <a href="https://en.wikipedia.org/wiki/Gilberto_Silva">Gilberto Silva</a>. He scored 3 league goals in their invincible season, but his primary duties were defensive. He had different characteristics than Thierry Henry, but there is some commonality to take advantage of here. If Gilberto Silva scores an opportunity that gives me <em>some</em> information about whether Thierry Henry could have scored it too. How much information? That depends on how similar they are. Unless we tell the model, it will assume we cannot learn anything about these players from the other. Both were professional footballers. Neither was a hockey player, or a tree, or a kitten - though a statistical model could not intuit this. If the data did indicate that they were in fact very different players, then the special model structure that we are using would recognise this and not share information between them to the same extent.</p>
<p>If the above concept make sense to you, then congratulations - you appreciate the utility of multi-level (partial-pooling) Bayesian models. This <em>sharing of information</em> is one of many reasons Bayesian methods can perform so well on small (and imperfect) datasets.</p>
</section>
<section id="what-does-the-model-do" class="level3">
<h3 class="anchored" data-anchor-id="what-does-the-model-do">What does the Model do?</h3>
<p>We have a model that describes uncertainty (using probability) and makes both team-level and player-specific predictions. Here are some examples:</p>
<p>How about <a href="https://en.wikipedia.org/wiki/Dennis_Bergkamp">Dennis Bergkamp’s</a> dinked finish when clear through on goal against Birmingham. Remember it? Me neither - here’s where the shot was taken from:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/xg_pt1/index_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>And our predicted xG is shown below, both for Dennis Bergkamp (upper) and for the case where any Arsenal player could be shooting (lower). Here is a great example of being able to make a better prediction conditional on the information of who is taking the shot. The model has identified that Bergkamp was very capable of scoring these kind of chances and was therefore able to identify a narrow range of very high xG values. However, if we were considering a generic player in the Arsenal team, there is more uncertainty in our prediction.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/xg_pt1/index_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>What about Thierry Henry’s long range goal against Man Utd? (Note that the straight arrow in the below plot does not reflect the true trajectory of his shot).</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/xg_pt1/index_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/xg_pt1/index_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>OK, so I wouldn’t have seen that one coming either ….but I would have given it more of a chance knowing who was shooting.</p>
<p>Here is a final example - a shot from Gilberto Silva, on his stronger foot, which was saved by Neil Sullivan (who I’d completely forgotten had <a href="http://news.bbc.co.uk/sport1/hi/football/teams/c/chelsea/3190149.stm">signed for Chelsea that season</a>). I thought this was worth looking at because StatsBomb’s xG suggests this was a very good chance.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/xg_pt1/index_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Our model did not expect him to score, and also predicted that <a href="https://en.wikipedia.org/wiki/Freddie_Ljungberg">Freddie Ljungberg</a> would have missed. Henry (unsurprisingly) is expected to have had a better chance, but <strong>interestingly</strong>, our model thinks that Arsenal’s goalscoring winger <a href="https://en.wikipedia.org/wiki/Robert_Pires">Robert Pirès</a> would have been most likely to score this opportunity.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/xg_pt1/index_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="final-thought-making-sense-of-probabilistic-predictions" class="level3">
<h3 class="anchored" data-anchor-id="final-thought-making-sense-of-probabilistic-predictions">Final Thought: Making Sense of Probabilistic Predictions</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/e78UID432cQGA/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>Thoughtful Pirlo</figcaption>
</figure>
</div>
<p>What should we make of the above predictions? The single values (<em>point estimates</em>) provided by analytics companies may be a bit easier to read, but I’m suggesting that they are not as useful. We should want our models to tell us when they are not sure. There is more information in a probabilistic prediction than a point estimate, which means you can go from the former to the latter, but not vice-versa. The type of model we have discussed in this article has the added benefit of sharing information between different players in a mathematically coherent way (see <a href="https://allyourbayes.com/posts/xg_pt2/">part 2</a>) for the technical details).</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{di_francesco2020,
  author = {Di Francesco, Domenic},
  title = {Uncertainty in {xG.} {Part} 1},
  date = {2020-12-10},
  url = {https://allyourbayes.com/posts/xg_pt1/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-di_francesco2020" class="csl-entry quarto-appendix-citeas">
Di Francesco, Domenic. 2020. <span>“Uncertainty in xG. Part 1.”</span>
December 10, 2020. <a href="https://allyourbayes.com/posts/xg_pt1/">https://allyourbayes.com/posts/xg_pt1/</a>.
</div></div></section></div> ]]></description>
  <category>football</category>
  <category>analysis</category>
  <category>xg</category>
  <category>Stan</category>
  <category>uncertainty</category>
  <category>Bayes</category>
  <category>multi-level modelling</category>
  <category>partial pooling</category>
  <guid>https://allyourbayes.com/posts/xg_pt1/</guid>
  <pubDate>Thu, 10 Dec 2020 00:00:00 GMT</pubDate>
  <media:content url="https://allyourbayes.com/posts/xg_pt1/xg1.png" medium="image" type="image/png" height="99" width="144"/>
</item>
<item>
  <title>Decisions under Uncertainty. Part 1</title>
  <dc:creator>Domenic Di Francesco</dc:creator>
  <link>https://allyourbayes.com/posts/BDA_pt1/</link>
  <description><![CDATA[ 






<section id="tldr" class="level3">
<h3 class="anchored" data-anchor-id="tldr">TLDR</h3>
<p>For many of you reading this, developing and testing predictive models will be a lot of fun - but my pragmatic, buzz-killing message here, is that we shouldn’t forget why we’re asked to do it. I suggest that the reason is (almost?) always, to help support decision-making. And yet, we often have this unhelpful disconnect between a calculation, and what the results imply for how we should act. An analyst hands over their work to a manager or budget holder, who then reinterprets everything and makes a call, which may or may not be consistent with the information they have been provided - sound familiar? It does to me.</p>
<p>In this post (Part 1) these two domains are considered jointly, using an example. We see how this can allow for practioners to arrive at coherent (quantitiative) and reproducible (auditable) decisions. Part 2 will consider a specific application of this approach, known as value of information analysis.</p>
<p>As ever, all code (in <code>Julia</code> today) is freely available!</p>
<hr>
</section>
<section id="a-familiar-scenario" class="level3">
<h3 class="anchored" data-anchor-id="a-familiar-scenario">A Familiar Scenario</h3>
<p>When I worked as an engineer I would often be asked to perform calculations (using spreadsheets <span class="emoji" data-emoji="upside_down_face">🙃</span> ) to arrive at some number regarding a safe operating condition for a damaged structure.</p>
<p>Models for the sake of themselves can be plenty of fun, but unless they are tied to the underlying decsions that they are supposed to inform, they are not necessarily fulfilling their purpose.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExZngxNnN0YzE5eWdmNDJua3JwYmN3dGJza2Y5d3l5eXo3YTFyZTlvbiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/miFGO0mIRDhf2/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>So You’ve Fit a Predictive Model?</figcaption>
</figure>
</div>
<p>How might we bridge this gap? How</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  d1[Decision] --&gt; o1((Uncertain \noutcome))
  d1 --&gt; c1{Utility}
  o1 --&gt; c1

</pre>
</div>
<p></p><figcaption> Example influence diagram with decisions (square nodes), uncertain outcomes (circles) and utility (diamonds).</figcaption> </figure><p></p>
</div>
</div>
</div>
</section>
<section id="analysis-vibes" class="level3">
<h3 class="anchored" data-anchor-id="analysis-vibes">Analysis &gt; Vibes!</h3>
<p>These (decision/influence/whatever!) diagrams provide a clear map of what factors we think we need to consider when making a decision. They also provide causal information, showing how intervening at one point can have downstream effects. They may seem trivial, but once you try and draw one for a problem you are working on, you will see they encourage you to think deeply about your area of expertise - no seriously, please try it!</p>
<p>Their apparent simplicity also has the benefit of being able to communicate qualitative information with various stakeholders. An experienced engineer could map out complex relationships between variables so that a statistical model could be developed that was consistent with this information. This could then be used to inform decisions.</p>
<p>But, is any of this ever required? If the goal is to arrive at a decision that aligns with subject matter knowledge, then can’t we just ask the expert to jump right to the decision?</p>
<p>This will not always be a valid shortcut. If you have attended workshops that were organised to get a room of experts to agree on a strategy, there will not always be a clear agreement. This is not necessarily because of a lack of expertise, but because of a lack of a common language to communicate the information. Formalising the structure of the problem, the prior knowledge, and where data should be used to update parameters results in a quantitive, reproducible and (with a little extra work) explanaible decision-making process.</p>
</section>
<section id="promising-vs.-delivering-an-example" class="level3">
<h3 class="anchored" data-anchor-id="promising-vs.-delivering-an-example">Promising vs.&nbsp;Delivering: An Example</h3>
<section id="intro" class="level4">
<h4 class="anchored" data-anchor-id="intro">Intro</h4>
<p>Lets consider a simple example…</p>
<p>An urn is filled with 10 balls, 5 red and 5 blue. As part of a lottery game, you are invited to draw a ball…nah, I’m just kidding. We’re not doing that. Let’s all please stop doing that.</p>
<p>You’re starting a business and have a prototype handmade product that is gaining some interest. You’ve pitched to some retailers and you have a couple of contracts on the table. However, these big shops are covering their backs with a penalty clause - if you deliver the order on time, you’ll be paid, if you over-promise (under-deliver) you pay a penalty to cover their ‘downstream’ losses.</p>
</section>
<section id="some-data" class="level4">
<h4 class="anchored" data-anchor-id="some-data">Some data</h4>
<p>To help make this decision, lets draw it out. Below is a diagram where we have listed all of the factors that we would like to consider, and how they are related.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  d1[Decision] --&gt; o1((Uncertain \noutcome))
  d1 --&gt; c1{Utility}
  o1 --&gt; c1
</pre>
</div>
<p></p><figcaption> Small business decision problem: which contract should you accept?</figcaption> </figure><p></p>
</div>
</div>
</div>
<p>…or not.</p>
<p>We may not necessarily have data on the rate at which you can produce your product. Focussing on decisions, rather than models reminds us of this. Sometimes a decision is required before we complete the experiments, trials, surveys etc. that we would like to.</p>
<p>Maybe you’ve not been feeling confident in yourself lately, and are too worried to take on any risk. Perhaps some well meaning friends have tried to counter this by telling you that you can achieve anything. This may convince you to take whatever orders are available. It’s easy to be swayed either way by such circumstances, and anecdotes and intuition drive decisions at much larger companies too.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># for working with data</span></span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">using</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">CSV</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">DataFrames</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">DataFramesMeta</span></span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># for fitting probabilistic models</span></span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">using</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Random</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Distributions</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">Turing</span></span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># for optimisation and solving decision problems</span></span>
<span id="cb1-8"></span>
<span id="cb1-9"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">using</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">JuMP</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">HiGHS</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">DecisionProgramming</span></span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[ Info: Precompiling LogDensityProblemsADForwardDiffBenchmarkToolsExt [501e9729-9453-5dfd-bf52-aa40e3634be9]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb3-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">MersenneTwister</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">231123</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|&gt;</span></span>
<span id="cb3-2">  prng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rand</span>(prng, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">Normal</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>10-element Vector{Float64}:
 -0.4797047482429591
  0.3492046160452603
  1.9035942889710349
 -1.0599399228488406
 -0.05026230935484321
  1.6159635845804887
  0.46192798081799236
  0.38816396593759556
 -1.4500821161955104
 -0.3667444519178413</code></pre>
</div>
</div>
</section>
</section>
<section id="reproducibility" class="level3">
<h3 class="anchored" data-anchor-id="reproducibility">Reproducibility</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExajFkdTI0dnN6amt1cmZ5Y3UwaHFneG5xb3cybXVwZXcxYjN3bDdvbCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/W1tZZhie5U7RYqjg7Z/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>Might your decisions impact the safety of the public?</figcaption>
</figure>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{di_francesco,
  author = {Di Francesco, Domenic},
  title = {Decisions Under {Uncertainty.} {Part} 1},
  date = {},
  url = {https://allyourbayes.com/posts/BDA_pt1/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-di_francesco" class="csl-entry quarto-appendix-citeas">
Di Francesco, Domenic. n.d. <span>“Decisions Under Uncertainty. Part
1.”</span> <a href="https://allyourbayes.com/posts/BDA_pt1/">https://allyourbayes.com/posts/BDA_pt1/</a>.
</div></div></section></div> ]]></description>
  <category>Decision Analysis</category>
  <category>Uncertainty</category>
  <category>Julia</category>
  <category>Optimisation</category>
  <category>Bayesian</category>
  <guid>https://allyourbayes.com/posts/BDA_pt1/</guid>
  <pubDate>Invalid Date</pubDate>
  <media:content url="https://allyourbayes.com/posts/BDA_pt1/young_ada.png" medium="image" type="image/png" height="148" width="144"/>
</item>
<item>
  <title>Player form. Part 1: Overview</title>
  <dc:creator>Domenic Di Francesco</dc:creator>
  <link>https://allyourbayes.com/posts/player_form/</link>
  <description><![CDATA[ 






<section id="tldr" class="level3">
<h3 class="anchored" data-anchor-id="tldr">TLDR</h3>
<p>When is a player <strong>in form</strong> (over performing, or enjoying a hot streak) and how long does this last? If there is such an effect, I suspect it will be a result of some complicated system of personal circumstances. In this post I suggest a popular statistical model (Gaussian process) for approximating the dependencies (how many games back should we look?) and non-linearities (rise and fall of form) that we need. Again, I am suggesting that we should care about uncertainty when trying to model just about anything in football, and using probability is a helpful way of doing so.</p>
<p>Ellen White’s data from the 2019-20 WSL season (courtesy of StatsBomb) is used as an example.</p>
<p>Similarly to the <a href="https://allyourbayes.com/posts/xg_pt2/">posts on multi-level models</a>, this will also be split into 2 parts. Part 1 (here) will focus on the features of a Gaussian process that are well suited to approximating player form. Part 2 (in preparation) will include more technical details and more code.</p>
<hr>
</section>
<section id="a-questionable-measure-of-player-form" class="level3">
<h3 class="anchored" data-anchor-id="a-questionable-measure-of-player-form">A (Questionable) Measure of Player Form</h3>
<p>As I alluded to in the TLDR above, I suspect a players form is somehow linked to their current mental state. When they are feeling confident they may be less likely to doubt their abilities, and more decisive. This could mean they act quicker and become more difficult to play against.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/STYcRDzNO6AYNmMgvA/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>Confidence is key …probably</figcaption>
</figure>
</div>
<p>I will not propose a detailed causal model here, just a statistical proxy. But, I will be assuming that form can rise and decay over time. For some players even a single good or bad performance may be enough to drastically impact their next game, and for others this process may be smoother and less volatile. More on this later.</p>
<p>For the purposes of this post, goalscoring form on a given match day, <img src="https://latex.codecogs.com/png.latex?i">, is defined as the difference between the number of goals that were scored on that match day, minus the expected number of goals, <img src="https://latex.codecogs.com/png.latex?xG"> associated with the opportunities in that game.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AForm_%7B%5Bi%5D%7D%20=%20Goals_%7B%5Bi%5D%7D%20-%20xG_%7B%5Bi%5D%7D%0A"></p>
<p>A nice feature of this is that, in principle, it is invariant to the quality of opposition. A striker may have a higher xG when performing against a weaker team, but will therefore need to score more goals in such a game to be considered in the same form. By the same token, it should also account for the fact that a player will generally get fewer scoring opportunities as a substitute.</p>
<p>…As for the not so nice features, there are plenty! For instance, what good is a measure of form that only considers goals scored? Is xG not also conditional on how well a striker is paying? Would it be more useful to standardise the result?</p>
<p>These are all fair questions, and with a little thought could all be integrated into a more comprehensive characterisation. However, the type of model that I will introduce will be equally compatible with alternative definitions, so let’s imagine we just care about whether a striker is scoring as many goals as they should be, and whether this will continue.</p>
</section>
<section id="ellen-white" class="level3">
<h3 class="anchored" data-anchor-id="ellen-white">Ellen White</h3>
<p>Ellen White is a clinical striker who, at the time of writing this, plays for Manchester City and England. She is a former winner of the Women’s Super League (WSL) golden boot, and is England’s all-time top scorer. So plenty of opportunities to see her distinctive celebration:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/SVlBW8O5jw3dtU17dv/giphy-downsized-large.gif" class="img-fluid figure-img"></p>
<figcaption>Ellen’s goal goggles</figcaption>
</figure>
</div>
<p>StatsBomb have kindly made data from the WSL (2019/20 season) freely available in <a href="https://github.com/statsbomb/StatsBombR">their R package</a>, and so we will consider this league season of Ellen White’s career here.</p>
<p>Here is a plot of Ellen’s <strong>form</strong> (performance vs.&nbsp;xG) over the 12 league games that she featured in, during that season. Her biggest over performance vs.&nbsp;xG was when she scored in a <a href="https://womenscompetitions.thefa.com/Article/TottenhamManCity05012020">4-1 win away at Tottenham</a> despite a cumulative xG of 0.597. Her worst performance by the same measure was the following week, failing to score in the <a href="https://womenscompetitions.thefa.com/Article/Manchester-City-3-1-Everton-110120">3-1 win at home to Everton</a>. Although she only played the final <img src="https://latex.codecogs.com/png.latex?25"> minutes of this game, she accrued a match xG of 0.302. The fact that these games were back to back could be tricky for a form model to accommodate!</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/player_form/index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="what-would-such-a-model-need-to-do" class="level3">
<h3 class="anchored" data-anchor-id="what-would-such-a-model-need-to-do">What would such a model need to do?</h3>
<p>Essentially, we are looking for are some numbers to help us understand the following:</p>
<ul>
<li>What form is a player currently in?</li>
<li>How long will a player remain in good (or bad) form?</li>
<li>What is the uncertainty in our predictions?</li>
</ul>
<p>Since players can enter good and bad patches of form over the course of a season, we need a model that is able to twist and turn accordingly. This means we need some <em>non-linearity</em>.</p>
<p>We also want future predictions to be based on recent games - if a player has over performed for the last 3 games in a row, then we generally expect them to continue on this path, at least in the short-term. But how far back should we look? Does a single great performance from <img src="https://latex.codecogs.com/png.latex?3"> months ago have any impact on a players current form? We need to quantify this <em>dependency</em> in our model too.</p>
<p>Finally, a probabilistic model has the benefit of <em>quantifying uncertainty</em>. I emphasise the importance of this in the ‘final thoughts’ at the end of this post and for anyone interested, here is <a href="https://www.allyourbayes.com/post/2020-03-24-why-go-bayesian/">more Bayesian statistics propaganda</a>. But, in summary we should not neglect uncertainty in this model because (a) We are not even sure what form is, and (b) we are estimating it from a small amount of indirect observations. So let’s not pretend we will end up with a single number. Enough preaching and back to the task at hand….</p>
<p>One solution that checks the above requirements is the Gaussian Process (GP).</p>
</section>
<section id="what-does-it-look-like" class="level3">
<h3 class="anchored" data-anchor-id="what-does-it-look-like">What does it look like?</h3>
<p>So we have this probabilistic model of smooth, non-linear functions. Let’s see what it looks like. In the below plot, the match days are the same as those presented in the above plot, we just have a new y-axis scale, and we have ‘days’ (rather than date) along the x-axis.</p>
<p>There are multiple functions that are consistent with Ellen White’s form in the league that season, so let’s look at one example first:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/player_form/index_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Where there is a large gap between successive games (such as the 3 weeks between White’s first and second appearances of the season), there is less evidence to guide predictions of form. This is also true for the period around day 60 (late December). Here though, she was on an upward trajectory. In both cases, this lack of data results in higher uncertainty, as is apparent when we look at more samples, which are shown on top of the full predictive distribution below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/player_form/index_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="and-so-what" class="level3">
<h3 class="anchored" data-anchor-id="and-so-what">And, so what?</h3>
<p>Some squiggly lines that approximately go through some points? What is the value of this when you could scribble something similar without knowing anything about statistics?</p>
<p>Well, underlying all of these lines is a model of dependency. We have quantified how similar (correlated) form should be in successive games, and how this correlation will decrease with time. I will talk about the parameters that do this and how they can be interpreted in part 2 (in preparation), but to summarise, the model quantifies how correlation in form decreases as time progresses and this can be seen in the smoothness of the lines.</p>
<p>For example, if form was always shown to be very similar to that of the previous game, then transitioning from good to bad performances would be gradual, and the samples from the associated GP model would be very smooth. Conversely, in the case where performance in subsequent games were completely independent, even if very little time had passed, the GP regression lines would need to be able to change direction very sharply.</p>
<p>Below are some predictions from the model in the period just after White’s last game of the season (to the right of the final match day on the above plots). She appeared to be on a slight upward trend at this point, over performing in her final game at home to Chelsea. This is shown in the uppermost histogram. As we move away from this game, into the off-season, we see the uncertainty gradually increase in our predictions and the average move towards zero. This is consistent with the considerations discussed above.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/player_form/index_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="some-final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="some-final-thoughts">Some final thoughts…</h3>
<section id="uncertainty" class="level4">
<h4 class="anchored" data-anchor-id="uncertainty">Uncertainty</h4>
<p>Any football models we propose will only vaguely resemble the ‘true’ data generating process and though we can incrementally add more parameters we do not automatically find more evidence for them. We can build big datasets by combining observations from multiple players, and leagues, but ignoring possible variation between such data is misleading. If your big football models need big data, why stop there? Feed it some Sunday league football, or some basketball, or some handwritten digits.</p>
<p>Alternatively, we acknowledge that our system of parameters are not perfectly precise, and our predictions will span credible ranges. Quantifying this variability is a strength, not a weakness of our models, and is actually of more direct use in decision support.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/3otPoyudZ18xASX0rK/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>Yes we can!</figcaption>
</figure>
</div>
</section>
<section id="sources-of-information" class="level4">
<h4 class="anchored" data-anchor-id="sources-of-information">Sources of Information</h4>
<p>What can we do about long periods in time where no competitive games are taking place?</p>
<p>There may be other sources of information that could help, such as performances in other competitions or even in training. Given we are not sure of the extent that these should inform the model, there is an argument to use a multi-level (partial pooling) structure, as was used to improve <a href="https://allyourbayes.com/posts/xg_pt2/">player-specifc xG estimates</a>.</p>
<p>Finally, the other source of information is that contained in the priors, which I have not included here. But don’t panic, some prior predictive sampling is on the way in part 2 (in preparation).</p>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{di_francesco2023,
  author = {Di Francesco, Domenic},
  title = {Player {Form.} {Part} 1},
  date = {2023-01-19},
  url = {https://allyourbayes.com/posts/player_form/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-di_francesco2023" class="csl-entry quarto-appendix-citeas">
Di Francesco, Domenic. 2023. <span>“Player Form. Part 1.”</span> January
19, 2023. <a href="https://allyourbayes.com/posts/player_form/">https://allyourbayes.com/posts/player_form/</a>.
</div></div></section></div> ]]></description>
  <category>football</category>
  <category>analysis</category>
  <category>Stan</category>
  <category>uncertainty</category>
  <category>Bayes</category>
  <category>gaussian process</category>
  <guid>https://allyourbayes.com/posts/player_form/</guid>
  <pubDate>Thu, 19 Jan 2023 00:00:00 GMT</pubDate>
  <media:content url="https://allyourbayes.com/posts/player_form/plot.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Why be Bayesian?</title>
  <dc:creator>Domenic Di Francesco</dc:creator>
  <link>https://allyourbayes.com/posts/Why_Bayes/</link>
  <description><![CDATA[ 






<section id="tldr" class="level3">
<h3 class="anchored" data-anchor-id="tldr">TLDR</h3>
<p>This post is intended to be a high-level discussion of the merits and challenges of applied Bayesian statistics. It is intended to help the reader answer: <em>Is it worth me learning Bayesian statistics?</em> or <em>Should I look into using Bayesian statistics in my project?</em> No maths or code in this post.</p>
<hr>
<section id="bayes" class="level4">
<h4 class="anchored" data-anchor-id="bayes">Bayes</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/TJBbXQooivUNq/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>Bayes</figcaption>
</figure>
</div>
</section>
</section>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>Firstly, Bayesian…</p>
<ul>
<li>Statistics</li>
<li>Inference</li>
<li>Modelling</li>
<li>Updating</li>
<li>Data Analysis</li>
</ul>
<p>…can boradly be considered the same thing (certainly for the purposes of this post): <strong>the application of Bayes theorem to quantify uncertainty</strong>.</p>
<p>Depending on your background, you may have preconceptions about Bayesian methods being specialist or complicated. However, analysts and researchers in many domains are increasingly experimenting with the growing catalogue of open source software and resources that is making probabilistic programming more accessible.</p>
</section>
<section id="what-does-a-bayesian-approach-provide" class="level3">
<h3 class="anchored" data-anchor-id="what-does-a-bayesian-approach-provide">What does a Bayesian approach provide?</h3>
<p>Bayesian statistics is not the only way to account for uncertainty in calculations. The below points describe what a Bayesian approach offers, that others don’t. Note that I am only really discussing methods involving probability here, though <a href="https://www.springer.com/gp/book/9783540402947">alternative approaches are available</a>.</p>
<section id="intuitive-interpretation-of-results" class="level4">
<h4 class="anchored" data-anchor-id="intuitive-interpretation-of-results">Intuitive Interpretation of Results</h4>
<p>Bayesian methods give you distributions. Your parameters (and so also your predictions) are all described as distributions. A single, joint distribution in fact, which is aligned with the evidence that you provide your model. This allows you to propagate all the uncertainties and inter-dependencies when making predictions for some new input data. By comparison, alternative (frequentist) methods typically describes uncertainty in predictions using confidence intervals, which are widely used but easy to misinterpret.</p>
<p>Confidence intervals are calculated so that they will contain the <em>true</em> value of whatever you are trying to predict with some desired frequency. They provide no information (in the absence of additional assumptions) on how credible various possible results are. The Bayesian equivalent (sometimes called credible intervals) can be drawn anywhere on a predictive distribution. In <a href="https://mitpress.mit.edu/books/introduction-statistical-decision-theory">Pratt, Raiffa and Schlaiffer’s textbook</a> an example is used to highlight this difference:</p>
<p><em>Imagine the plight of the manager who exclaims, ‘I understand [does he?] the meaning that the demand for XYZ will lie in the interval 973 to 1374 with confidence .90. However, I am particularly interested in the interval 1300 to 1500. What confidence can I place on that interval?’</em> <em>Unfortunately, this question cannot be answered. Of course, however, it is possible to give a posterior probability to that particular interval - or any other - based on the sample data and on a codification of the manager’s prior judgements.</em></p>
<p>This is a nice example (aside from assuming the manager is a male) of a simple question that we need a Bayesian posterior distribution to answer. A more succinct description of the same view from <a href="https://www.weirdfishes.blog/">Dan Ovando’s fishery statistics blog</a>:</p>
<p><em>Bayesian credible intervals mean what we’d like Frequentist confidence intervals to mean.</em></p>
</section>
<section id="seamless-integration-with-decision-analysis" class="level4">
<h4 class="anchored" data-anchor-id="seamless-integration-with-decision-analysis">Seamless Integration with Decision Analysis</h4>
<p>Following on from the previous point, an analysis that directly describes the probability of any outcome is fully compatible with a decision analysis. After completing a Bayesian analysis, identifying the optimal strategy implied by your model becomes simpler and more understandable.</p>
<p>As stated in <a href="https://www.springer.com/gp/book/9780387960982">James Berger’s (quite theoretical) book on Bayesian statistics</a>:</p>
<p><em>Bayesian analysis and decision theory go rather naturally together, partly because of their common goal of utilizing non-experimental sources of information, and partly because of deep theoretical ties.</em></p>
</section>
<section id="flexible-modelling" class="level4">
<h4 class="anchored" data-anchor-id="flexible-modelling">Flexible Modelling</h4>
<p>So this one is based on a point made in <a href="https://uk.sagepub.com/en-gb/eur/a-student%E2%80%99s-guide-to-bayesian-statistics/book245409">Ben Lambert’s book on Bayesian statistics</a>. It is regarding how modern Bayesian statistics is achieved in practice. The computational methods may require some effort to pick up, especially if you do not have experience with programming (though Ben’s book gives a nice introduction to <a href="https://mc-stan.org/">Stan</a>). However, they can be readily extended to larger and more complex models.</p>
<p>Essentially, we can propose weird and bespoke model structures, and then use the same approach to fit them.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/WiyczarN2XMm4/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>Some Compelling Arguments</figcaption>
</figure>
</div>
</section>
</section>
<section id="challenges-difficulties" class="level3">
<h3 class="anchored" data-anchor-id="challenges-difficulties">Challenges &amp; Difficulties</h3>
<p>So why would anyone ever <em>not</em> use Bayesian models when making predictions?</p>
<section id="subjectivity" class="level4">
<h4 class="anchored" data-anchor-id="subjectivity">Subjectivity</h4>
<p>Perhaps the most common criticism of Bayesian statistics is the requirement for priors - i.e.&nbsp;a starting point for your model. This initial estimate of uncertainty is a term in Bayes’ theorem - but how can you estimate the extent of variability before you see it in your data? This will surely be completely subjective, so the results will vary depending on who is doing the analysis. This, understandably, doesn’t seem right with a lot of casual enquirers.</p>
<p>A common response to this accusation is that subjectivity is not an exclusive feature of Bayesian analysis (how about the whole approach you are taking to solving your problem !) <em>…but</em> at least Bayesians are required to be explicit about it. Priors mean that any subjective inputs have no-where to hide (in the code or the reporting) and so they are open to criticism. This point is discussed in <strong>much</strong> more detail in this paper from <a href="http://www.stat.columbia.edu/~gelman/research/published/gelman_hennig_full_discussion.pdf">Colombia University</a>.</p>
<p>Priors can contain, as much or as little, information as desired. However, even in instances where you may feel you don’t have any upfront knowledge of a problem, they represent a valuable opportunity for introducing regularisation (which protects against bad predictions due to overfitting). This idea is discussed in detail in <a href="https://www.crcpress.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919">Richard McElreath’s textbook</a>. I would suggest that in plenty of cases, we often have a fair idea of what is happening even before we collect and analyse data, and starting from zero everytime is just not sensible. Will a doctor who has just evaluated you base their diagnosis entirely on a test outcome, or will they incorporate that eveidence into their existing understanding of your circumstances and symptoms? Bayesian statistics is the means by which this can be done mathematically.</p>
</section>
<section id="computational-requirements" class="level4">
<h4 class="anchored" data-anchor-id="computational-requirements">Computational Requirements</h4>
<p>In practice, statisticians estimate Bayesian posterior distributions using very clever Markov Chain Monte Carlo (MCMC) sampling algorithms, that are run by their favourite probabilistic programming software. The models that I have worked with during my PhD have taken several hours to finish sampling from, but I have met statisticians whose models run for days or even weeks. Following this, there are checks that need to be completed as there are plenty of things that can go wrong with MCMC. There is a nice discussion of a recommended Bayesian workflow to make sure you are checking what you need to <a href="https://arxiv.org/abs/2011.01808">here</a>.</p>
<p>My background is in mechanical and civil engineering. In discussions with engineering researchers at conferences I have often been told that the errors and complications they encountered when playing around with Bayesian software caused them to abandon the approach in favour of more established, less informative analysis. These are challenges that I imagine everyone who has attempted modern Bayesian statistics will have encountered and resolving them can require a deep understanding of your model, and perhaps some formal training. In addition some programming <em>tricks</em> like reparameterisation (describing your model in a seemingly equivalent way, but one that is much friendlier to your software) can also help.</p>
</section>
</section>
<section id="conclusions" class="level3">
<h3 class="anchored" data-anchor-id="conclusions">Conclusions</h3>
<p>Regardless of whether you believe we exist in a deterministic universe or not, you will never have perfect state of knowledge describing your problem: uncertainty exists, so we need a sensible and safe way of accounting for it. It’s difficult to escape the implications of the Maths - for instance, in avoiding quantifying uncertainty, we are often making some implicit assumptions (the implications of which can be difficult to justify) that we may not want to propagate into our decision-making. There is generally a trade-off for apparent conveniences.</p>
<p>I believe that Bayesian statistics is actually well suited to traditional engineering problems, which are concerned with managing risk when confronted with small, messy datasets and models with plenty of uncertainty. As suggested in the earlier description of confidence intervals, frequentist statistics defines probability based on occurrences of events following a large number of trials or samples. When trying to understand the behaviour of small datasets (or even unique structural systems), Bayesian statistics can shine by comparison.</p>
<p>Very large datasets may contain enough information to precisely estimate parameters in a model using conventional machine learning methods, and so it becomes less worthwhile running simulations to characterise variability. But how common are these big data problems in science and engineering? Sometimes large populations of data are better described as multiple smaller constituent groups, after accounting for key differences between them. Bayesian statistics has a very useful way of managing such problems by structuring models hierarchically. This method allows for <strong>partial pooling of information</strong> between groups, so that predictions account for the variability and commonality between groups. I will provide a detailed example of this in a future post.</p>
<p>Bayesian inference requires (computational and personal) effort to apply. But it provides results that are generally more interpretable and closely related to the actual questions we want to answer. Whether or not these methods are worth learning will of course depend on personal circumstances. I encountered this field during my PhD, and so had plenty of time to read and play with them, which I appreicate is a privelidged position to be in.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/WPLPEu0GUp41W/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>Boring, isn’t it? Writing, Fitting and Evaluating Bayesian Models All Day….</figcaption>
</figure>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{di_francesco2020,
  author = {Di Francesco, Domenic},
  title = {Why Be {Bayesian?}},
  date = {2020-03-24},
  url = {https://allyourbayes.com/posts/Why_Bayes/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-di_francesco2020" class="csl-entry quarto-appendix-citeas">
Di Francesco, Domenic. 2020. <span>“Why Be Bayesian?”</span> March 24,
2020. <a href="https://allyourbayes.com/posts/Why_Bayes/">https://allyourbayes.com/posts/Why_Bayes/</a>.
</div></div></section></div> ]]></description>
  <category>Bayes</category>
  <category>uncertainty</category>
  <category>decisions</category>
  <guid>https://allyourbayes.com/posts/Why_Bayes/</guid>
  <pubDate>Tue, 24 Mar 2020 00:00:00 GMT</pubDate>
  <media:content url="https://allyourbayes.com/posts/Why_Bayes/gp.png" medium="image" type="image/png" height="84" width="144"/>
</item>
<item>
  <title>Bayesian Logistic Regression with Stan</title>
  <dc:creator>Domenic Di Francesco</dc:creator>
  <link>https://allyourbayes.com/posts/Logistic_Bayes/</link>
  <description><![CDATA[ 






<section id="tldr" class="level3">
<h3 class="anchored" data-anchor-id="tldr">TLDR</h3>
<p>Logistic regression is a popular statistical model for making predictions on outcomes between zero and one - like probabilities. One application of it in an engineering context is quantifying the effectiveness of inspection technologies at detecting damage. This post describes the additional information provided by a Bayesian application of logistic regression (and how it can be implemented using the <code>Stan</code> probabilistic programming language). I’ve also included some recommendations for giving your model a sensible starting point (using prior information).</p>
<hr>
</section>
<section id="introductions" class="level3">
<h3 class="anchored" data-anchor-id="introductions">Introductions</h3>
<p>So there are a couple of key topics discussed here: logistic regression, and Bayesian inference. Before jumping straight into the example application, I’ve provided some <strong>very</strong> brief introductions below.</p>
<section id="bayesian-inference" class="level4">
<h4 class="anchored" data-anchor-id="bayesian-inference">Bayesian Inference</h4>
<p>At a very high level, Bayesian models quantify uncertainty, so that our predictions and decisions take into account the ways in which our knowledge is limited or imperfect. We specify a statistical model, and identify probabilistic estimates for the parameters. My preferred software for writing a fitting Bayesian models is <a href="https://mc-stan.org/"><code>Stan</code></a>. If you are not yet familiar with Bayesian statistics, then I imagine you won’t be fully satisfied with that 3 sentence summary, so I will put together a separate post on the merits and challenges of applied Bayesian inference, which will include much more detail.</p>
</section>
<section id="logistic-regression" class="level4">
<h4 class="anchored" data-anchor-id="logistic-regression">Logistic Regression</h4>
<p>Logistic regression is used to estimate the probability of a binary outcome, such as <em>Pass</em> or <em>Fail</em> (though it can be extended for <code>&gt; 2</code> outcomes). This is achieved by transforming a standard regression using the logit function, shown below. The term in the brackets may be familiar to gamblers as it is how odds are calculated from probabilities (p). You may see <em>logit</em> and <em>log-odds</em> used exchangeably for this reason.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextrm%7BLog%20odds%7D%20=%20%5Clog%5CBigg(%7B%5Cfrac%7Bp%7D%7B1%20-%20p%7D%7D%5CBigg)%0A"></p>
<p>Since the logit function transformed data <em>from</em> a probability scale, the inverse logit (or logistic) function transforms data <em>to</em> a probability scale. We can re-arrange this equation ourselves to see this:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cexp(%5Ctextrm%7BLog%20odds%7D)%20=%20%5CBigg(%7B%5Cfrac%7Bp%7D%7B1%20-%20p%7D%7D%5CBigg)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cexp(%5Ctextrm%7BLog%20odds%7D)%20%5Ctimes%20(1%20-%20p)%20=%20p%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cexp(%5Ctextrm%7BLog%20odds%7D)%20-%20p%20%5Ctimes%20%5Cexp(%5Ctextrm%7BLog%20odds%7D)%20=%20p%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cexp(%5Ctextrm%7BLog%20odds%7D)%20=%20p%20+%20p%20%5Ctimes%20%5Cexp(%5Ctextrm%7BLog%20odds%7D)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cexp(%5Ctextrm%7BLog%20odds%7D)%20=%20p%20(1%20+%20%5Cexp(%5Ctextrm%7BLog%20odds%7D))%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cexp(%5Ctextrm%7BLog%20odds%7D)%7D%20%7B1%20+%20%5Cexp(%5Ctextrm%7BLog%20odds%7D)%7D%20=%20p%0A"></p>
<p>because multiplying by <img src="https://latex.codecogs.com/png.latex?%5Cexp(x)"> is the same as diviving by <img src="https://latex.codecogs.com/png.latex?%5Cexp(-x)">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%20%7B%5Cexp(%5Ctextrm%7B-Log%20odds%7D)%20%5Ctimes%20(1%20+%20%5Cexp(%5Ctextrm%7BLog%20odds%7D))%7D%20=%20p%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%20%7B%5Cexp(%5Ctextrm%7B-Log%20odds%7D)%20%20+%201%7D%20=%20p%0A"></p>
<p>As shown in the below plot, the values of this function range from <code>0</code> to <code>1</code>.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextrm%7BInverse%20Logit%7D%20(x)%20=%20%5Cfrac%7B1%7D%7B1%20+%20%5Cexp(-x)%7D%0A"></p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/Logistic_Bayes/index_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The reason we start with with log-odds is because we can define a linear model on this scale, with a gradient, <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> and an intercept <img src="https://latex.codecogs.com/png.latex?%5Calpha">, and then transform it to a prediction on the probability scale, using the above steps. When a linear regression is combined with a re-scaling function such as this, it is known as a Generalised Linear Model (<strong>GLM</strong>).</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextrm%7BLog%20odds%7D%20=%20%5Calpha%20+%20%5Cbeta%20%5Ctimes%20%5Ctextrm%7Binput%20variables%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap%20=%20%5Cfrac%7B1%7D%7B1%20+%20%5Cexp(-%5B%5Calpha%20+%20%5Cbeta%20%5Ctimes%20%5Ctextrm%7Binput%20variables%7D%5D)%7D%0A"></p>
<p>You may be familiar with libraries that automate the fitting of logistic regression models, either in <code>Python</code> (via <code>sklearn</code>):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LogisticRegression</span>
<span id="cb1-2"></span>
<span id="cb1-3">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LogisticRegression()</span>
<span id="cb1-4">model.fit(X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dataset[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'input_variables'</span>], y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dataset[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'predictions'</span>])</span></code></pre></div>
</div>
<p>…or in <code>R</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">model_fit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">glm</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">formula =</span> preditions <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> input_variables,</span>
<span id="cb2-2">                 <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> dataset, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">binomial</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">link =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logit'</span>))</span></code></pre></div>
</div>
</section>
</section>
<section id="example-application-probability-of-detection" class="level3">
<h3 class="anchored" data-anchor-id="example-application-probability-of-detection">Example Application: Probability of Detection</h3>
<p>To demonstrate how a Bayesian logistic regression model can be fit (and utilised), I’ve included an example from one of my papers. Engineers make use of data from inspections to understand the condition of structures. Modern inspection methods, whether remote, autonomous or manual application of sensor technologies, are very good. They are generally evaluated in terms of the accuracy and reliability with which they size damage. Engineers never receive perfect information from an inspection, such as:</p>
<ul>
<li>There is a crack of <strong>exact</strong> length <code>30 mm</code> and <strong>exact</strong> depth <code>5 mm</code> at this <strong>exact</strong> location, or</li>
<li>There is <strong>definitely</strong> no damage at this location.</li>
</ul>
<p>For various reasons, the information we receive from inspections is imperfect and this is something that engineers need to deal with. As a result, providers of inspection services should be requested to provide some measure of how good their product is. This typically includes some measure of how accurately damage is sized and how reliable an outcome (detection or no detection) is.</p>
<p>This example will consider trials of an inspection tool looking for damage of varying size, to fit a model that will predict the probability of detection for any size of damage. Since various forms of damage can initiate in structures, each requiring inspection methods that are suitable, let’s avoid ambiguity and imagine we are only looking for cracks.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/42wQXwITfQbDGKqUP7/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>Detecting damage: never 100% reliable</figcaption>
</figure>
</div>
<section id="test-data" class="level4">
<h4 class="anchored" data-anchor-id="test-data">Test Data</h4>
<p>For the purposes of this example we will simulate some data. Let’s imagine we have introduced some cracks (of known size) into some test specimens and then arranged for some blind trials to test whether an inspection technology is able to detect them.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">set.seed</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1008</span>)</span>
<span id="cb3-2"></span>
<span id="cb3-3">N <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>; lower <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>; upper <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>; alpha_true <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>; beta_true <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb3-4"></span>
<span id="cb3-5">depth <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">runif</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">n =</span> N, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">min =</span> lower, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">max =</span> upper)</span>
<span id="cb3-6"></span>
<span id="cb3-7">PoD_1D <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(depth, alpha_1D, beta_1D){</span>
<span id="cb3-8">  PoD <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">exp</span>(alpha_1D <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> beta_1D <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(depth)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">exp</span>(alpha_1D <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> beta_1D <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(depth)))</span>
<span id="cb3-9">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">return</span> (PoD)</span>
<span id="cb3-10">}</span>
<span id="cb3-11"></span>
<span id="cb3-12">pod_df <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tibble</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">depth =</span> depth, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">det =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">double</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">length =</span> N))</span>
<span id="cb3-13"></span>
<span id="cb3-14"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> (i <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">seq</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">from =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">to =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(pod_df), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">by =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)) {</span>
<span id="cb3-15">  </span>
<span id="cb3-16">  pod_df<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>det[i] <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rbernoulli</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">n =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, </span>
<span id="cb3-17">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">p =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">PoD_1D</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">depth =</span> pod_df<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>depth[i], </span>
<span id="cb3-18">                                       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha_1D =</span> alpha_true, </span>
<span id="cb3-19">                                       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">beta_1D =</span> beta_true))</span>
<span id="cb3-20">  </span>
<span id="cb3-21">}</span></code></pre></div>
</div>
<p>The above code is used to create 30 crack sizes (depths) between 0 and 10 mm. We then use a log-odds model to back calculate a probability of detection for each. This is based on some fixed values for <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. In a real trial, these would not be known, but since we are inventing the data we can see how successful our model ends up being in estimating these values.</p>
<p>The below plot shows the size of each crack, and whether or not it was detected (in our simulation). The smallest crack that was detected was 2.22 mm deep, and the largest undetected crack was 5.69 mm deep. Even so, it’s already clear that larger cracks are more likely to be detected than smaller cracks, though that’s just about all we can say at this stage.</p>
<p>After fitting our model, we will be able to predict the probability of detection for a crack of any size.</p>
<div class="cell" data-ouput="false">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/Logistic_Bayes/index_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><code>Stan</code> is a <a href="https://en.wikipedia.org/wiki/Probabilistic_programming">probabilistic programming language</a>. In a future post I will explain why it has been my preferred software for statistical inference throughout my PhD.</p>
<p>The below is a simple <code>Stan</code> program to fit a Bayesian Probability of Detection (PoD) model:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">PoD_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> cmdstanr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cmdstan_model</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">stan_file =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'PoD_model.stan'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in readLines(stan_file): incomplete final line found on
'PoD_model.stan'</code></pre>
</div>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">PoD_model<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">print</span>()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>data {

  int &lt;lower = 0&gt; N; // Defining the number of defects in the test dataset
  array [N] int &lt;lower = 0, upper = 1&gt; det; // A variable that describes whether each defect was detected (1) or not (0)
  vector &lt;lower = 0&gt; [N] depth; // A variable that describes the corresponding depth of each defect
  
  int &lt;lower = 0&gt; K; // Defining the number of probabilistic predictions required from the model
  vector &lt;lower = 0&gt; [K] depth_pred;
  
}

parameters {
  
  // The (unobserved) model parameters that we want to recover
  real alpha;
  real beta;
  
}

model {

  // A logistic regression model relating the defect depth to whether it will be detected
  det ~ bernoulli_logit(alpha + beta * log(depth));
  
  // Prior models for the unobserved parameters
  alpha ~ normal(0, 1);
  beta ~ normal(1, 1);

}

generated quantities {
  
  // Using the fitted model for probabilistic prediction.
  // K posterior predictive distributions will be estimated for a corresponding crack depth
  vector [K] postpred_pr;
  
  for (k in 1:K) {
    
    postpred_pr[k] = inv_logit(alpha + beta * log(depth_pred[k]));
    
  }
  
}</code></pre>
</div>
</div>
<p>The <code>generated quantities</code> block will be used to make predictions for the <code>K</code> values of <code>depth_pred</code> that we provide.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1">K <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>; depth_pred <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">seq</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">from =</span> lower, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">to =</span> upper, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">length.out =</span> K)</span></code></pre></div>
</div>
<p>The above code generates 50 evenly spaced values, which we will eventually combine in a plot. In some instances we may have specific values that we want to generate probabilistic predictions for, and this can be achieved in the same way.</p>
</section>
<section id="fitting-the-model" class="level4">
<h4 class="anchored" data-anchor-id="fitting-the-model">Fitting the model</h4>
<p>Data can be pre-processed in any language for which a <code>Stan</code> interface has been developed. This includes, <code>R</code>, <code>Python</code>, and <code>Julia</code>. In this example we will use <code>R</code> and the accompanying package, <code>cmdstanr</code>.</p>
<p>Our <code>Stan</code> model is expecting data for three variables: <strong>N</strong>, <strong>det</strong>, <strong>depth</strong>, <strong>K</strong> and <strong>depth_pred</strong> and <code>cmdstanr</code> requires this in the form of a list.</p>
</section>
<section id="results" class="level4">
<h4 class="anchored" data-anchor-id="results">Results</h4>
<p>Once we have our data, and are happy with our model, we can set off the Markov chains. There are plenty of opportunities to control the way that the <code>Stan</code> algorithm will run, but I won’t include that here, rather we will mostly stick with the default arguments in <code>cmdstanr</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1">PoD_fit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> PoD_model<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sample</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">N =</span> N, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">det =</span> pod_df<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>det, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">depth =</span> pod_df<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>depth,</span>
<span id="cb9-2">                                        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">K =</span> K, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">depth_pred =</span> depth_pred), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">seed =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2408</span>)</span></code></pre></div>
</div>
<p><strong>Note</strong>:I’ve not included any detail here on the checks we need to do on our samples. There are some common challenges associated with MCMC methods, each with plenty of associated guidance on how to diagnose and resolve them. For now, let’s assume everything has gone to plan.</p>
<p>Now, there are a few options for extracting samples from a stanfit object such as <code>PoD_samples</code>, including <code>cmdstanr::as_draws()</code>. However, these usually require a little post-processing to get them into a tidy format. There is a function in my <a href="https://github.com/DomDF/DomDF">DomDF R package</a> for this, which we can use to create a tidy output that specifies the iteration, parameter value and chain associated with each data point:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1">PoD_samples <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> PoD_fit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|&gt;</span> DomDF<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tidy_mcmc_draws</span>()</span>
<span id="cb10-2"></span>
<span id="cb10-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">head</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> PoD_samples, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">n =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 4
  Parameter Chain Iteration value
  &lt;chr&gt;     &lt;int&gt;     &lt;int&gt; &lt;dbl&gt;
1 lp__          1         1 -15.6
2 lp__          1         2 -15.2
3 lp__          1         3 -15.5
4 lp__          1         4 -16.0
5 lp__          1         5 -16.2</code></pre>
</div>
</div>
<p>We have sampled from a 2-dimensional posterior distribution of the unobserved parameters in the model: <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. Below is a density plot of their corresponding marginal distributions based on the <code>1000</code> samples collected from each of the <code>4</code> Markov chains that have been run.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/Logistic_Bayes/index_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>So our estimates are beginning to converge on the values that were used to generate the data, but this plot also shows that there is still plenty of uncertainty in the results. Unlike many alternative approaches, Bayesian models account for the statistical uncertainty associated with our limited dataset - remember that we are estimating these values from 30 trials. These results describe the possible values of <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> in our model that are consistent with the limited available evidence. If more data was available, we could expect the uncertainty in our results to decrease. I think there are some great reasons to keep track of this statistical (sometimes called <em>epistemic</em>) uncertainty - a primary example being that we should be interested in how confident our predictive models are in their own results! …but I’ll leave it at that for now, and try to stay on topic.</p>
<p>How do we know what do these estimates of <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> mean for the PoD (what we are ultimately interested in)? We can check this using the posterior predictive distributions that we have (thanks to the <code>generated quantities</code> block of the <code>Stan</code> program).</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/Logistic_Bayes/index_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>One thing to note from these results is that the model is able to make much more confident predictions for larger crack sizes. The increased uncertainty associated with shallow cracks reflects the lack of data available in this region - this could be useful information for a decision maker!</p>
<p>There are only 3 trials in our dataset considering cracks shallower than 3 mm (and only 1 for crack depths <code>&lt; 2</code> mm). If we needed to make predictions for shallow cracks, this analysis could be extended to quantify the value of future tests in this region.</p>
</section>
<section id="final-thought-where-did-those-priors-come-from-and-are-they-any-good" class="level4">
<h4 class="anchored" data-anchor-id="final-thought-where-did-those-priors-come-from-and-are-they-any-good">Final Thought: Where Did Those Priors Come From and Are They Any Good?</h4>
<p>There are many approaches for specifying prior models in Bayesian statistics. <em>Weakly informative</em> and <em>MaxEnt</em> priors are advocated by various authors. Unfortunately, <em>Flat Priors</em> are sometimes proposed too, particularly (but not exclusively) in older books. A flat prior is a wide distribution - in the extreme this would be a uniform distribution across all real numbers, but in practice distribution functions with very large variance parameters are sometimes used. In either case, a very large range prior of credible outcomes for our parameters is introduced the model. This may sound innocent enough, and in many cases could be harmless.</p>
<p>Flat priors have the appeal of describing a state of complete uncertainty, which we may believe we are in before seeing any data - but is this really the case?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://media.giphy.com/media/UgM7H8OEmf4mQ/giphy.gif" class="img-fluid figure-img"></p>
<figcaption>Prior Expectations: Can We Do Better?</figcaption>
</figure>
</div>
<p>Suppose you are using Bayesian methods to model the speed of some athletes. Even before seeing any data, there is some information that we can build into the model. For instance, we can discount negative speeds. We also wouldn’t need to know anything about the athletes to know that they would not be travelling faster than the speed of light. This may sound facetious, but flat priors are implying that we should treat all outcomes as equally likely. In fact, there are some cases where flat priors cause models to require large amounts of data to make good predictions (meaning we are failing to take advantage of Bayesian statistics ability to work with limited data).</p>
<p>In this example, we would probably just want to constrain outcomes to the range of metres per second, but the amount of information we choose to include is ultimately a modelling choice. Another helpful feature of Bayesian models is that the priors are part of the model, and so must be made explicit - fully visible and ready to be scrutinised.</p>
<p>A common challenge, which was evident in the above PoD example, is lacking an intuitive understanding of the meaning of our model parameters. Here <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> required prior models, but I don’t think there is an obvious way to relate their values to the result we were interested in. They are linear regression parameters on a log-odds scale, but this is then transformed into a probability scale using the logit function.</p>
<p>This problem can be addressed using a process known as <strong>Prior Predictive Simulation</strong>, which I was first introduced to in <a href="https://www.crcpress.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919">Richard McElreath’s fantastic book</a>. This involves evaluating the predictions that our model would make, based only on the information in our priors. Relating our predictions to our parameters provides a clearer understanding of the implications of our priors.</p>
<p>Back to our PoD parameters - both <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> can take positive or negative values, but I could not immediately tell you a sensible range for them. Based on our lack of intuition it may be tempting to use a variance for both, right? Well, before making that decision, we can always simulate some predictions from these priors. The below code is creating a data frame of prior predictions for the PoD (<code>PoD_pr</code>) for many possible crack sizes.</p>
<p><em>(Thank you to Jiun for your kind message that helped me tidy up the below)</em></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha%20%5Csim%20N(%5Cmu_%7B%5Calpha%7D,%20%5Csigma_%7B%5Calpha%7D)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbeta%20%5Csim%20N(%5Cmu_%7B%5Cbeta%7D,%20%5Csigma_%7B%5Cbeta%7D)%0A"></p>
<p>And we can visualise the information contained within our priors for a couple of different cases.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/Logistic_Bayes/index_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Our wide, supposedly <em>non</em>-informative priors result in some pretty useless predictions. I’ve suggested some more sensible priors that suggest that larger cracks are more likely to be detected than small cracks, without overly constraining our outcome (see that there is still prior credible that very small cracks are detected reliably and that very large cracks are often missed).</p>
<p>Why did our predictions end up looking like this?</p>
<p>Borrowing from McElreath’s explanation, it’s because <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> are linear regression parameters on a log-odds (logit) scale. Since we are estimating a PoD we end up transforming out predictions onto a probability scale. Flat priors for our parameters imply that extreme values of log-odds are credible. All that prior credibility of values <code>&lt; - 3</code> and <code>&gt; 3</code> ends up getting concentrated at probabilities near <code>0</code> and <code>1</code>. I think this is a really good example of flat priors containing a lot more information than they appear to.</p>
<p>I’ll end by directing you towards some additional (generally non-technical) discussion of choosing priors, written by the <code>Stan</code> development team <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">(link)</a>. It provides a definition of <em>weakly informative priors</em>, some words of warning against <em>flat priors</em> and more general detail than this humble footnote.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://allyourbayes.com/posts/Logistic_Bayes/index_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{di_francesco2020,
  author = {Di Francesco, Domenic},
  title = {Bayesian {Logistic} {Regression} with {Stan}},
  date = {2020-02-15},
  url = {https://allyourbayes.com/posts/Logistic_Bayes/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-di_francesco2020" class="csl-entry quarto-appendix-citeas">
Di Francesco, Domenic. 2020. <span>“Bayesian Logistic Regression with
Stan.”</span> February 15, 2020. <a href="https://allyourbayes.com/posts/Logistic_Bayes/">https://allyourbayes.com/posts/Logistic_Bayes/</a>.
</div></div></section></div> ]]></description>
  <category>Bayes</category>
  <category>Logistic Regression</category>
  <category>MCMC</category>
  <category>Stan</category>
  <category>R</category>
  <guid>https://allyourbayes.com/posts/Logistic_Bayes/</guid>
  <pubDate>Sat, 15 Feb 2020 00:00:00 GMT</pubDate>
  <media:content url="https://allyourbayes.com/posts/Logistic_Bayes/logistic_priors.png" medium="image" type="image/png" height="108" width="144"/>
</item>
</channel>
</rss>
