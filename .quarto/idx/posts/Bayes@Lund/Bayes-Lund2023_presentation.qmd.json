{"title":"Identifying Data Requirements using Bayesian Decision Analysis","markdown":{"yaml":{"title":"Identifying Data Requirements using Bayesian Decision Analysis","subtitle":"Bayes@Lund2023","execute":{"echo":true,"message":false,"warning":false},"format":{"revealjs":{"progress":true,"chalkboard":false,"preview-links":true,"theme":["simple","custom.scss"],"smaller":true,"scrollable":true,"transition":"slide","institute":"The Alan Turing Institute <br> CSML Research Group, Civil Engineering, University of Cambridge","author":"Domenic Di Francesco, PhD, CEng (MIMechE)","code-fold":true,"date":"23 January 2023","date-format":"full"}}},"headingText":"TLDR","containsRefs":false,"markdown":"\n\n\nThese are the slides I presented at the Bayes@Lund2023 conference in January. I spoke about identifying data requirements (for engineering systems) using value of information analysis.\n\n## Uncertainty and Decisions\n\n```{r}\n#| echo: false\nlibrary(tidyverse); setwd(\"~/Github/Bayes-Lund2023/Bayes-Lund2023_presentation\")\n```\n\n::: {.incremental}\n - *\"Many if not most statistical analyses are performed for the ultimate goal of decision making.\"*\n <br> Andrew Gelman et. al, [Bayesian Data Analysis](https://doi.org/10.1201/b16018). <br>\n\n - *\"Bayesian analysis and decision theory go rather naturally together, partly because of their common goal of utilizing non-experimental sources of information, and partly because of deep theoretical ties.\"* \n <br> Prof. James Berger, [Statistical Decision Theory and Bayesian Analysis](https://doi.org/10.1007/978-1-4757-4286-2). <br>\n\n - *\"It is important that all problems of inference be visualized as problems of decision.\"*\n<br> Prof. Ian Jordaan, [Decisions Under Uncertainty](https://doi.org/10.1017/CBO9780511804861).<br>\n\n - *\"It (statistics) is seldom really appreciated for what it can be used for, namely as a basis for assessing information and organising the process of acquiring knowledge in pursuit of supporting decision making.\"* \n <br> Prof. Michael Faber, [Statistics and Probability Theory: In Pursuit of Engineering Decision Support](https://doi.org/10.1007/978-94-007-4056-3). <br>\n\n:::\n\n## Uncertainty and Decisions\n\n::: panel-tabset\n### Decision-event trees\n\n![Example decision tree](Figures/decision_tree.png)\n\n### Influence diagrams\n\n![Example influence diagram](Figures/influence_diagram.png)\n\nSelect action $a^{*}$ associated with the highest expected utility:\n\n```{=tex}\n\\begin{equation}\na^{*} = \\arg \\max_{a \\in A} \\mathop{\\mathbb{E}}_{\\theta \\sim \\pi(\\theta)} \\big[ u(a, \\theta) \\big]\n\\end{equation}\n```\n\n### Value of Information\n\n![Example influence diagram](Figures/VoI.png)\n\n::: {style=\"text-align: center\"}\n\n*How* and *to what extent* will this data facilitate improved decision making?\n\n```{=tex}\n\\begin{equation}\nVoI(e_{i}) = \\mathop{\\mathbb{E}}_{\\theta \\sim \\pi(\\theta), \\\\z \\sim f(z \\mid \\theta)} \\big[ u(e_{i}, z, a^{*}, \\pi(\\theta \\mid z) \\big] - \\mathop{\\mathbb{E}}_{\\theta \\sim \\pi(\\theta)} \\big[ u( a^{*}, \\pi(\\theta)) \\big]\n\\end{equation}\n```\n\n:::\n\n:::\n\n## Examples\n\n```{julia}\n# For working with data\nusing CSV, DataFrames, DataFramesMeta, RCall\n\n# For describing probabilistic models\nusing Distributions, Random, LatinHypercubeSampling, StanSample, MCMCChains\n\n# For describing and solving decision problem\nusing JuMP, HiGHS, DecisionProgramming, LinearAlgebra\n\n```\n\n```{julia}\n#| echo: false\n#| output: false\n\nset_cmdstan_home!(\"/Users/ddifrancesco/.cmdstan/cmdstan-2.30.1\"); CMDSTAN_HOME\n\n```\n\n## Building Ventilation\n\n::: panel-tabset\n\n### Problem\n\nSpecify the ventilation setting for an office building: \n\n - Low\n - Standard\n - Well Ventilated\n - High\n\nHigher ventilation rates decrease the risk of infection from airborne disease, but cost more to run.\n\n[Link to paper](https://doi.org/10.1098/rspa.2020.0584)\n\n### Influence diagram\n\n![Influence diagram for building ventilation problem](Figures/occupancy.png)\n\n### Model\n\n```{julia}\n#| output: false\ns = 3600; venting = [1, 3, 5, 10] ./ s; κ = 0.39 / s; λ = 0.636 / s\nloss_rate = venting .+ κ .+ λ; tₘ = 8 * s; n_step = 100\n\nloss_rate = venting .+ κ .+ λ; tₘ = 8 * 3600; n_step = 100\n\nfunction Pr_infection(occupancy::Int64, Vol::Float64, loss::Float64, Δt = tₘ / (n_step - 1), infection_rate = 0.02, iᵣ = 5.21 * 10^-4, Cᵣ = 410, Nᵣ = 0.453)\n    C = zeros(n_step); nᵢₙₕ = zeros(n_step); Prᵢ = zeros(n_step); t = zeros(n_step); \n    N = occupancy * infection_rate\n    for i in 2:(n_step)\n        t[i] = t[i-1] + Δt\n        C[i] = N * Nᵣ / (Vol * loss) + (C[i-1] - N * Nᵣ / (Vol * loss)) * exp(-1 * loss * Δt)\n        nᵢₙₕ[i] = nᵢₙₕ[i-1] + iᵣ * Δt * C[i]\n        Prᵢ[i] = 1 - exp(-1 * nᵢₙₕ[i] / Cᵣ)\n    end\n    return last(Prᵢ)\nend\n\nfunction pr_inf_pop(pr_inf::Float64, occupants::Int64)\n    return Binomial(occupants, pr_inf) |> x -> pdf.(x, collect(0:occupants))\nend\n\nfunction draw_lhs(dist, n::Int)\n    samples = randomLHC(n + 2, 1) |>\n        x -> scaleLHC(x, [(0, 1)])[:, 1] |>\n        x -> filter(∉((0, 1)), x) |>\n        x -> sort(x) |>\n        x -> quantile(dist, x)\n    return samples\nend\n\n```\n\n### Prior uncertainty\n\n```{julia}\n#| output: false\nλpr = 30; n_samples = 1000\noccupancy_model = Poisson(λpr); occupancy_samples = draw_lhs(occupancy_model, n_samples)\n\nvent_options = Dict(\"Low\" => 5, \"Standard\" => 30, \"Well_Ventilated\" => 45, \"High\" => 90)\nvent_states = keys(vent_options) |> x -> collect(x)\nvent_costs = [vent_options[state] for state in vent_states]\n\ninfection_states = [\"Infected\", \"Uninfected\"]\n\nsickness_costs = Dict(\"Sick_Day\" => 115 * 3)\n```\n\n```{julia}\n#| echo: false\n#| output: false\n@rput(occupancy_samples)\n```\n\n```{r}\n#| echo: false\nggplot(data = tibble(o = occupancy_samples))+\n  geom_histogram(mapping = aes(x = o, y = after_stat(x = density)), \n                 binwidth = 3, col = \"black\", alpha = 1/3)+\n  scale_x_continuous(name = \"Number of Occupants\")+\n  scale_y_continuous(name = \"Prior Likelihood\")+\n  ggthemes::theme_base(base_family = \"Atkinson Hyperlegible\", base_size = 12)+\n  theme(plot.background = element_rect(colour = NA),\n        legend.title = element_blank(), legend.position = \"top\")\n```\n\n### Decision analysis\n\n```{julia}\nfunction exp_opt_venting(occupants::Vector{Int64} = occupancy_samples, optimiser = HiGHS.Optimizer)\n\n    occupancy_states = append!([\"0\"], string.(occupants))\n\n    # Initialise the influence diagram\n    occupancy_decision = InfluenceDiagram()\n\n    # Create structure of influence diagram\n    add_node!(occupancy_decision, DecisionNode(\"Ventilation\", [], vent_states))\n    add_node!(occupancy_decision, ChanceNode(\"Occupancy\", [], occupancy_states))\n    add_node!(occupancy_decision, ValueNode(\"Cost_Infection\", [\"Ventilation\", \"Occupancy\"]))\n    add_node!(occupancy_decision, ValueNode(\"Cost_Ventilation\", [\"Ventilation\"]))\n\n    generate_arcs!(occupancy_decision)\n\n    # Calculate the probability of infection for each ventilation option\n    pr_inf = [Pr_infection.(o, 2000.0, loss_rate) for o in occupants] |>\n        x -> [v[i] for i in 1:4 for v in x] |>\n        x -> [x[1:length(occupants)], \n              x[length(occupants)+1:2*length(occupants)], \n              x[2*length(occupants)+1:3*length(occupants)], \n              x[3*length(occupants)+1:4*length(occupants)]]\n    pr_low = pr_inf[1]; pr_std = pr_inf[2]; pr_well = pr_inf[3]; pr_high = pr_inf[4]\n\n    # Calculate the probability of each possible number of total infections in the building\n    # ...and the associated expected costs due to sickness\n    infection_df = DataFrame(occupants = occupants, \n                             pr_inf_low = pr_inf_pop.(pr_low, occupants),\n                             pr_inf_std = pr_inf_pop.(pr_std, occupants),\n                             pr_inf_well = pr_inf_pop.(pr_well, occupants),\n                             pr_inf_high = pr_inf_pop.(pr_high, occupants)) |>\n        x -> @rtransform(x, :cost_sick = [sickness_costs[\"Sick_Day\"] * o for o in reverse(:occupants:-1:0)]) |>\n        x -> @rtransform(x, :EC_low = :pr_inf_low .* :cost_sick |> x -> sum(x)) |>\n        x -> @rtransform(x, :EC_std = :pr_inf_std .* :cost_sick |> x -> sum(x)) |>\n        x -> @rtransform(x, :EC_well = :pr_inf_well .* :cost_sick |> x -> sum(x)) |>\n        x -> @rtransform(x, :EC_high = :pr_inf_high .* :cost_sick |> x -> sum(x)) |>\n        x -> @rselect(x, :occupants, :EC_low, :EC_std, :EC_well, :EC_high)\n\n    # Assigning probabilities and utilities to diagram\n    Pr_occ = ProbabilityMatrix(occupancy_decision, \"Occupancy\")\n    C_inf = UtilityMatrix(occupancy_decision, \"Cost_Infection\")\n    C_vent = UtilityMatrix(occupancy_decision, \"Cost_Ventilation\")\n\n    Pr_occ = append!([0.0], repeat([1/length(occupants)], length(occupants)))\n\n    C_inf[\"Low\", :] = append!([0.0], infection_df.EC_low)\n    C_inf[\"Standard\", :] = append!([0.0], infection_df.EC_std)\n    C_inf[\"Well_Ventilated\", :] = append!([0.0], infection_df.EC_well)\n    C_inf[\"High\", :] = append!([0.0], infection_df.EC_high)\n\n    C_vent[\"Low\"] = vent_options[\"Low\"]; C_vent[\"Standard\"] = vent_options[\"Standard\"]\n    C_vent[\"Well_Ventilated\"] = vent_options[\"Well_Ventilated\"]; C_vent[\"High\"] = vent_options[\"High\"]\n\n    add_probabilities!(occupancy_decision, \"Occupancy\", Pr_occ)\n    add_utilities!(occupancy_decision, \"Cost_Infection\", C_inf)\n    add_utilities!(occupancy_decision, \"Cost_Ventilation\", C_vent)\n\n    # generate the full influence diagram\n    generate_diagram!(occupancy_decision)\n\n    # Define and run solver\n    decision_model = JuMP.Model(optimiser); set_silent(decision_model)\n\n    z = DecisionVariables(decision_model, occupancy_decision)\n    x_s = PathCompatibilityVariables(decision_model, occupancy_decision, z)\n\n    Exp_Cost = expected_value(decision_model, occupancy_decision, x_s)\n\n    @objective(decision_model, Min, Exp_Cost)\n    optimize!(decision_model)\n\n    # Process results\n    Z = DecisionStrategy(z)\n    U_dist = UtilityDistribution(occupancy_decision, DecisionStrategy(z))\n\n    exp_opt_decision = DataFrame(u_opt = LinearAlgebra.dot(U_dist.p, U_dist.u),\n                                 a_opt = vent_states[argmax(Z.Z_d[1])])\n\n    return exp_opt_decision\n    \nend\n\nprior_ventilation_decision = exp_opt_venting()\n\n```\n:::\n\n## Building Ventilation\n\n#### Expected value of measuring occupancy\n\n```{julia}\n#| output: false\n\nmeasure_occupancy_df = DataFrame()\nfor o in occupancy_samples\n    append!(measure_occupancy_df, exp_opt_venting([o]))\nend\n\nprior_cost = prior_ventilation_decision.u_opt[1] \nprepost_cost = mean(measure_occupancy_df.u_opt)\n\nVoPI = prior_cost - prepost_cost\n```\n\n```{julia}\n#| echo: false\n#| output: false\n@rput(prior_ventilation_decision); @rput(measure_occupancy_df)\n```\n\n```{r}\n#| echo: false\n#| fig-cap: \"Expected cost and optimal action associated with each simulation from a prospective measurement of building occupancy\"\n\nmeasure_occupancy_df$meas <- occupancy_samples\nmeasure_occupancy_df$a_opt <- gsub(pattern = \"Well_Ventilated\", replacement = \"Well \\nVentilated\", x = measure_occupancy_df$a_opt)\n\narrow_df <- tibble(xend = prior_ventilation_decision$u_opt,\n                   x = measure_occupancy_df$u_opt |> mean(),\n                   y = 4, yend = 4)\n\nggplot(data = bind_rows(measure_occupancy_df, tibble(a_opt = \"High\", u_opt = NA)) |>\n         mutate(a_opt = factor(x = a_opt, levels = c(\"Low\", \"Standard\", \"Well \\nVentilated\", \"High\"))),\n                        mapping = aes(x = u_opt, y = a_opt))+\n  geom_jitter(shape = 21, alpha = 1/2, height = 1/3, mapping = aes(fill = meas))+\n  geom_vline(mapping = aes(xintercept = prior_ventilation_decision$u_opt, \n                           lty = \"Expected Cost Without Measuring Occupancy\"), alpha = 1/2) +\n  geom_vline(mapping = aes(xintercept = measure_occupancy_df$u_opt |> mean(), \n                           lty = \"Expected Cost Measuring Occupancy\"), alpha = 1/2)+\n  geom_segment(data = arrow_df,\n               mapping = aes(x = x, xend = xend, y = y, yend = yend, col = \"EVoPI\"), \n               arrow = arrow(length = unit(0.25, \"cm\"), ends = \"first\", type = \"closed\"))+\n  scale_color_manual(values = c(\"midnightblue\"))+\n  scale_fill_viridis_c()+\n  labs(y = \"Expected Optimal Action\", fill = \"Measured Occupancy\", lty = \"\", col = \"\")+\n    scale_linetype_manual(values = c(2, 1))+\n  scale_x_continuous(name = \"Expected Cost\", limits = c(0, 125), labels = scales::dollar_format(), breaks = scales::pretty_breaks())+\n  ggthemes::theme_base(base_size = 14, base_family = \"Atkinson Hyperlegible\")+\n  theme(legend.position = 'top', legend.title = element_text(size = 10), axis.text.y = element_text(angle = 90, hjust = 0.5), \n        plot.background = element_rect(colour = NA))+\n  guides(linetype = guide_legend(nrow = 2), \n         fill = guide_colorbar(title.position = 'top', barwidth = 10, barheight = 1/2, order = 3))\n```\n\n## Inspecting for Corrosion\n\n::: {.panel-tabset}\n\n### Problem\n\nIdentify a repair plan for $10$ locations of corrosion damage:\n\n```{=tex}\n\\begin{equation}\nCGR = \\dfrac{d_{i2} - d_{i1}}{T_{i2} - T_{i1}}\n\\end{equation}\n```\n\n - $2$ inspections have been completed\n - Inspection $2$ was incomplete\n\n - Should the inspection team return to location $4$?\n \n [Link to paper](https://doi.org/10.1017/dce.2021.18)\n\n### Influence diagram\n\n![Influnce diagram for inspecting for corrosion](Figures/corrosion.png)\n\n### Model\n\n```{julia}\n#| echo: false\n#| output: false\n\ninspection_df = CSV.read(\"data_files/inspection_data.csv\", DataFrame)\nyears = unique(inspection_df.t); insps = unique(inspection_df.inspection); locations = unique(inspection_df.location)\n\nfunction lnorm_params(μ::Float64, σ::Float64)\n    sdlog = √(log(1 + σ^2 / μ^2))\n    meanlog = log(μ) - 0.5 * sdlog^2\n    return Dict(\"sdlog\" => sdlog, \"meanlog\" => meanlog)\nend\n\nprior_depth = lnorm_params(10.0, 6.0)\n\nmodel_text = open(\"data_files/corr_fp_md.stan\") do file\n    read(file, String)\nend\n\nmodel_data = Dict(\n    \"N\" => nrow(inspection_df),\n  \"n_A\" => unique(inspection_df.anomaly_id) |> x -> length(x),\n  \"n_M\" => sum(inspection_df.missing),\n  \"ID\" => inspection_df.anomaly_id,\n  \"depth_i1\" => inspection_df |> x -> @rsubset(x, :t == years[1]) |> x -> x.depth_mm,\n  \"depth_i2\" => inspection_df |> x -> @rsubset(x, :t == years[2]) |> x -> x.depth_mm,\n  \"error_i1\" => inspection_df |> x -> @rsubset(x, :t == years[1]) |> x -> x.sizing_uncertainty,\n  \"error_i2\" => inspection_df |> x -> @rsubset(x, :t == years[2]) |> x -> x.sizing_uncertainty,\n  \"d_years\" => maximum(years) - minimum(years),\n  \"ex_1\" => inspection_df |> x -> @rsubset(x, :t == years[1]) |> x -> x.missing,\n  \"ex_2\" => inspection_df |> x -> @rsubset(x, :t == years[2]) |> x -> x.missing,\n  \"mu_mu_beta\" => 1, \"sigma_mu_beta\" => 1, \"rate_sigma_beta\" => 1,\n  \"mu_depth_imp\" => prior_depth[\"meanlog\"],\n  \"sigma_depth_imp\" => prior_depth[\"sdlog\"]\n)\n\ntmpdir = pwd() * \"/tmp\"; model = SampleModel(\"corrosion_model\", model_text, tmpdir)\n\nn_chains = 4; n_warmup = 2_000; n_draws = Int(1_000 / n_chains)\n\nrc = model |>\n     x -> stan_sample(x; data = model_data, save_warmup = false, \n     num_warmups = n_warmup, num_samples = n_draws, thin = 1, delta = 0.85)\n\nif success(rc)\n    samples_df = read_samples(model, :mcmcchains) |> x -> DataFrame(x) |> x -> DataFrames.stack(x)\n    diags_df = read_summary(model) |> x -> DataFrame(x)\nend\n\ndepth_df = samples_df |>\n    x -> @rsubset(x, occursin(\"depth_true\", :variable)) |>\n    x -> @rtransform(x, :insp = occursin(\"i1\", :variable) ? \"Inspection 1\" : \"Inspection 2\") |>\n    x -> @rtransform(x, :anomaly = split(:variable, \".\") |> x -> \"Anomaly \" * x[2])\n\n@rput(samples_df); @rput(depth_df)\n```\n\n```stan\nfunctions{\n  real log_norm_sigma(real norm_mu, real norm_sigma){ \n    return log(1 + (norm_sigma^2 / norm_mu^2)); \n  }\n  \n  real log_norm_mu(real norm_mu, real norm_sigma){\n    return log(norm_mu) - 0.5 * log_norm_sigma(norm_mu, norm_sigma)^2;\n  }\n}\n\ndata {\n  int <lower = 1> N; // Number of data points\n  int <lower = 1> n_A; // Number of anomalies\n  int <lower = 0> n_M; // Number of anomalies missed\n\n  int <lower = 1> ID [N]; // Defect identifier\n\n  vector [n_A] depth_i1; // Measured corrosion depth\n  vector [n_A] depth_i2; // Measured corrosion depth\n  vector <lower = 0> [n_A] error_i1; // Measurement error parameter\n  vector <lower = 0> [n_A] error_i2; // Measurement error parameter\n\n  real d_years; // Time of measurement\n  \n  real mu_mu_beta; // Prior corrosion growth rate parameter\n  real <lower = 0> sigma_mu_beta; // Prior corrosion growth rate parameter\n  real <lower = 0> rate_sigma_beta; // Prior corrosion growth rate parameter\n  \n  real mu_depth_imp;  // Prior on missing data\n  real <lower = 0> sigma_depth_imp;  // Prior on missing data\n}\n\nparameters {\n  real mu_beta;\n  real <lower = 0> sigma_beta;\n  \n  vector <lower = 0> [n_A] depth_true_i1;\n  vector <lower = 0> [n_A] delta_depth;\n}\n\ntransformed parameters {\n  vector [n_A] depth_true_i2 = depth_true_i1 + delta_depth;\n  vector [n_A] growth; // Growth rate depth of corrosion\n  \n  for (i in 1:n_A){  \n    growth[i] = (depth_true_i2[i] - depth_true_i1[i]) / (d_years);\n  }\n}\n\nmodel {\n  // Model\n  for (n in 1:n_A) {\n    if (ex_1[n] == 0){\n      depth_i1[n] ~ normal(depth_true_i1[n], error_i1[n]);\n    }\n    \n    if (ex_1[n] == 1) {\n      depth_true_i1[n] ~ lognormal(mu_depth_imp, sigma_depth_imp);\n    }\n    \n    if (ex_2[n] == 0){\n      depth_i2[n] ~ normal(depth_true_i2[n], error_i2[n]);\n    }\n    \n    if (ex_2[n] == 1) {\n      depth_true_i2[n] ~ lognormal(mu_depth_imp, sigma_depth_imp);\n    }\n  }\n  growth ~ lognormal(mu_beta, sigma_beta);\n\n  // Priors\n  target += normal_lpdf(mu_beta | mu_mu_beta, sigma_mu_beta);\n  target += exponential_lpdf(sigma_beta | rate_sigma_beta);\n}\n\ngenerated quantities {\n   real CGR_pp = lognormal_rng(mu_beta, sigma_beta);\n}\n\n```\n\n### Prior uncertainty\n\n```{r}\n#| echo: false\n#| fig-cap: \"Estimated extent of corrosion damage for first 4 locations\"\n\ndepth_df <- depth_df |>\n  mutate(type = case_when(\n    (insp == \"Inspection 2\" & anomaly == \"Anomaly 4\") ~ \"Imputed\",\n    T ~ \"Measured\"\n  ))\n\nggplot(data = depth_df |> dplyr::filter(anomaly %in% (depth_df$anomaly |> unique())[1:4]))+\n  geom_density(mapping = aes(x = value, y = after_stat(x = density), alpha = type), \n               col = \"black\", fill = \"grey\")+\n  facet_grid(anomaly ~ insp)+\n  scale_x_continuous(name = \"Corrosion depth, mm\", breaks = scales::pretty_breaks())+\n  scale_y_continuous(name = \"Likelihood\", breaks = scales::pretty_breaks())+\n  ggthemes::theme_base(base_size = 14, base_family = \"Atkinson Hyperlegible\")+\n  theme(legend.position = 'top', legend.title = element_blank(), \n        plot.background = element_rect(colour = NA))\n\n```\n\n:::\n\n## Inspecting for Corrosion\n\n#### Expected value of completing inspection\n\n::: panel-tabset\n\n### Prior decision analysis\n```{r}\n#| echo: false\n\nVoInsp_prior <- read_csv(\"data_files/VoInsp_prior.csv\", )\n\nkableExtra::kable(VoInsp_prior)\n```\n\n### Preposterior decision analysis\n\n```{r}\n#| echo: false\n\nVoInsp <- read_csv(\"data_files/VoInsp.csv\")\n\narrow_df <- tibble(xend = sum(VoInsp_prior$Cost),\n                   x = VoInsp$cost |> mean(),\n                   y = 2e-4, yend = 2e-4)\n\nVoI <- sum(VoInsp_prior$Cost) -mean(VoInsp$cost)\n\nggplot(data = VoInsp) +\n  geom_histogram(mapping = aes(x = cost, y = after_stat(x = density)),\n                 col = \"black\", fill = \"grey\")+\n  geom_vline(mapping = aes(xintercept = mean(VoInsp$cost), lty = \"Expected cost following \\ninspection of Anomaly 4\"), alpha = 2/3)+\n  geom_vline(mapping = aes(xintercept = sum(VoInsp_prior$Cost), lty = \"Expected cost without \\ninspecting Anomaly 4\"), alpha = 2/3)+\n  geom_segment(data = arrow_df,\n               mapping = aes(x = x, xend = xend, y = y, yend = yend, col = \"Expected Value \\nof Inspection\"), \n               arrow = arrow(length = unit(0.25, \"cm\"), ends = \"first\", type = \"closed\"))+\n  geom_text(data = arrow_df, mapping = aes(x = mean(c(x, xend)), y = y + 2e-5, label = paste(\"$\", round(VoI, digits = 0))), family = \"Atkinson Hyperlegible\", size = 3.75)+\n  scale_color_manual(values = \"forestgreen\")+\n  scale_linetype_manual(values = c(2, 1))+\n  scale_x_continuous(name = \"Cost\", labels = scales::dollar_format(), breaks = scales::pretty_breaks())+\n  scale_y_continuous(name = \"Likelihood\")+\n  ggthemes::theme_base(base_size = 14, base_family = \"Atkinson Hyperlegible\")+\n  theme(legend.position = 'top', legend.title = element_blank(), \n        plot.background = element_rect(colour = NA))\n\n```\n\n:::\n\n## Some concluding thoughts\n\n- Explictly relate Bayesian models to underlying decisions:\n    - Consistent and coherent results on an intereptable scale\n    - Quantify expected value of collecting data\n\n- Challenges:\n    - Defining problem, e.g. identifying sources of value.\n    - Computation/combinatorics...\n\n- Further work...\n\n## Thank you for your attention!\n\n::: {style=\"text-align: center\"}\n\n<br>\n\n{{< fa envelope size=2x >}}\n[ddifrancesco@turing.ac.uk](ddifrancesco@turing.ac.uk)\n\n<br>\n\n{{< fa brands twitter size=2x >}} \n[@DomenicDF](https://twitter.com/Domenic_DF)\n\n<br>\n\n{{< fa brands github size=2x >}}\n[@DomDF](https://github.com/DomDF)\n\n:::\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","output-file":"Bayes-Lund2023_presentation.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.3.119","auto-stretch":true,"title-block-banner":true,"title":"Identifying Data Requirements using Bayesian Decision Analysis","subtitle":"Bayes@Lund2023","progress":true,"chalkboard":false,"previewLinks":true,"theme":["simple","custom.scss"],"smaller":true,"scrollable":true,"transition":"slide","institute":"The Alan Turing Institute <br> CSML Research Group, Civil Engineering, University of Cambridge","author":"Domenic Di Francesco, PhD, CEng (MIMechE)","date":"23 January 2023","date-format":"full"}}}}