{"title":"Uncertainty in xG. Part 2","markdown":{"yaml":{"title":"Uncertainty in xG. Part 2","subtitle":"Partially (Optimally) Pooling Hierarchical Data","author":"Domenic Di Francesco","date":"2021-01-07","categories":["football","analysis","xg","Stan","uncertainty","Bayes","multi-level modelling","partial pooling"],"image":"xg2.png","citation":true},"headingText":"TLDR","containsRefs":false,"markdown":"\n\n```{r}\n#| echo: false\n#| warning: false\n#| message: false\n\nknitr::opts_chunk$set(collapse = TRUE)\n```\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\n\nlibrary(extrafont); library(tidyverse); library(DomDF)\n\nlibrary(magick)\nSB_logo <- magick::image_read(path = \"SB_logo.png\")\n\nlibrary(StatsBombR)\n\nget_player_xG <- function(i, mcmc_df, model_data_df){\n\n  log_odds <- (mcmc_df$alpha + \n                 mcmc_df$beta_dist_goal * model_data_df$DistToGoal[i] +\n                 mcmc_df$beta_dist_keeper * model_data_df$DistToKeeper[i] +\n                 mcmc_df$beta_angle_dev * model_data_df$AngleDeviation[i] +\n                 mcmc_df$beta_angle_goal * model_data_df$AngleToGoal[i] +\n                 mcmc_df$beta_cone_def * model_data_df$DefendersInCone[i] +\n                 mcmc_df$beta_pressure * model_data_df$under_pressure[i] +\n                 mcmc_df$beta_with_head * model_data_df$shot.body_part.name_processed_Head[i] +\n                 mcmc_df$beta_with_weak_foot * model_data_df$shot.body_part.name_processed_Weaker.Foot[i])\n  \n  player <- model_data_df$player.name[i]; pred_player <- mcmc_df$player[i]\n\n  results_df <- data.frame(xG = (exp(x = log_odds) / (1 + exp(x = log_odds))), \n                           player = player,\n                           pred_player = pred_player) %>% \n    as_tibble()\n  \n  return(results_df)\n  \n}\n\nget_team_xG <- function(i, mcmc_df, model_data_df, n_prob = 1e3){\n  \n  log_odds <- (mcmc_df$alpha_pp + \n                 mcmc_df$beta_dist_goal_pp * model_data_df$DistToGoal[i] +\n                 mcmc_df$beta_dist_keeper_pp * model_data_df$DistToKeeper[i] +\n                 mcmc_df$beta_angle_dev_pp * model_data_df$AngleDeviation[i] +\n                 mcmc_df$beta_angle_goal_pp * model_data_df$AngleToGoal[i] +\n                 mcmc_df$beta_cone_def_pp * model_data_df$DefendersInCone[i] +\n                 mcmc_df$beta_pressure_pp * model_data_df$under_pressure[i] +\n                 mcmc_df$beta_with_head_pp * model_data_df$shot.body_part.name_processed_Head[i] +\n                 mcmc_df$beta_with_weak_foot_pp * model_data_df$shot.body_part.name_processed_Weaker.Foot[i])\n  \n  player <- model_data_df$player.name[i]\n  \n  results_df <- data.frame(xG = (exp(x = log_odds) / (1 + exp(x = log_odds))), \n                           player = player,\n                           pred_player = 'Unknown Player') %>% \n    as_tibble()\n  \n  return(results_df)\n  \n}\n\nAFC_goals_df <- read_csv(file = 'AFC_goals_df.csv', col_names = T, col_select = -c(...1))\n\nAFC_df <- read_csv(file = 'xG_model_data.csv', col_names = T, col_select = -c(...1))\n\nxG_mcmc_samples <- read_csv(file = 'xG_mcmc_samples.csv') |> select(c(Iteration, Chain, Parameter, value))\n\nxG_model_data_processed <- read_csv(file = 'model_data_processed.csv', col_names = T, col_select = -c(...1))\n\nany_player_df <- xG_mcmc_samples %>% \n  dplyr::filter(grepl(pattern = '_pp', x = Parameter)) %>% \n  tidyr::pivot_wider(names_from = Parameter, values_from = value)\n\nRP_df <- xG_mcmc_samples %>% \n  dplyr::filter(grepl(pattern = '[5]', x = Parameter)) %>% \n  mutate(Parameter = str_remove(string = Parameter, pattern = '.5]')) %>% \n  tidyr::pivot_wider(names_from = Parameter, values_from = value) %>% \n  mutate(player = unique(xG_model_data_processed$player.name)[5])\n\n```\n\n\nThis is part 2 of an article on fitting a Bayesian partial pooling model to predict expected goals. It has the benefits of (a) quantifying *aleatory and epistemic* uncertainty, and (b) making both group-level (player-specific) and population-level (team-specific) probabilistic predictions. If you are interested in these ideas but not in statistical language, then you can also check out [part 1](https://allyourbayes.com/posts/xg_pt1/).\n\n---\n\n### Expected Goals\n\nExpected Goals (or *xG*) is a metric that was developed to predict the probability of a football (soccer) player scoring a goal, conditional on some mathematical characterisation of the shooting opportunity. Since we have a binary outcome (he or she will either score or not score) we can use everyone's favourite GLM - logistic regression.\n\nUnfortunately this causes some overlap with a [previous blog post - '*Bayesian Logistic Regression with Stan*'](https://www.allyourbayes.com/post/2020-02-14-bayesian-logistic-regression-with-stan/), but don't worry - the focus here is all about *Partial Pooling*.\n\nFirst let's look at a non-Bayesian base case. [StatsBomb](https://statsbomb.com/) have kindly made lots of football data freely available in [their R package](https://github.com/statsbomb/StatsBombR). The below creates a dataframe of the shots taken by Arsenal FC during the `2003`-`04` Premier League winning season.\n\n```{r}\n#| echo: true\n#| eval: false\n#| warning: false\nlibrary(StatsBombR); library(tidyverse)\n\nPrem_SB_matches <- FreeMatches(Competitions = SB_comps %>% \n                               dplyr::filter(competition_name == 'Premier League') %>% \n                               dplyr::filter(competition_gender == 'male'))\n\nArsenal_0304_shots <- StatsBombFreeEvents(MatchesDF = Prem_SB_matches, \n                                          Parallel = TRUE) %>% \n  allclean() %>% \n  dplyr::filter(type.name == 'Shot') %>% \n  dplyr::filter(possession_team.name == 'Arsenal')\n\n```\n\nUsing `R`'s `tidymodels` framework - make sure to have a look at [Julia Silge's tutorials](https://www.youtube.com/channel/UCTTBgWyJl2HrrhQOOc710kA) if you are unfamiliar - we can specify and fit a logistic regression. The below compares our results (including confidence intervals) to those from StatsBomb. \n\nIf you are interested in creating something similar yourself, this model has standardised inputs for parameters with relatively large values (such as angles and distances) and one hot encoding of categorical inputs (such as whether or not the shot was taken with a players weaker foot).\n\n```{r}\n#| echo: false\n#| warning: false\n\npred_df <- read_csv('pred_df.csv', col_names = T, col_select = -c(...1)) %>%\n  mutate(goal_desc = case_when(\n    goal == 0 ~ 'Shot Missed',\n    goal == 1 ~ 'Shot Scored'\n  ))\n\nggplot(data = pred_df)+\n  geom_point(mapping = aes(x = .pred_1, y = shot.statsbomb_xg),\n             size = 3, alpha = 0.4)+\n  geom_errorbar(mapping = aes(y = shot.statsbomb_xg,\n                              xmin = .pred_lower_1, xmax = .pred_upper_1),\n                width = 0.01, alpha = 0.2)+\n  facet_wrap(facets = ~ goal_desc)+\n  labs(y = 'StatsBomb xG', x = 'tidymodels xG')+\n  geom_vline(xintercept = 0.5, col = 'firebrick', alpha = 0.4, lty = 2)+\n  geom_hline(yintercept = 0.5, col = 'firebrick', alpha = 0.4, lty = 2)+\n  DomDF::theme_ddf_light(base_family = \"Atkinson Hyperlegible\")\n\n```\n\nSince we have used StatsBomb data (though their model will no doubt be based on a much larger collection) we would expect our results to be similar to theirs, and they are. Considering just the point estimates, the two models appear to broadly agree, especially when both are predicting a very low or a very high xG.\n\nHowever, some of the confidence intervals on our `tidymodels` predictions are very large. Although we would generally expect these to decrease as we introduced more data, we know that football matches (and especially specific events within football matches) are full of uncertainty. If we want to be able to quantify this uncertainty in a more useful way (we do) - we want a Bayesian model. The below section details the specific type of Bayesian model that I'm proposing for estimating xG.\n\n### Multi-Level (Partial Pooling) Models\n\nHierarchical (or 'nested') data contains multiple groups within a population, such as players with a football team. Unfortunately, this information is lost (and bias is introduced) when such data is modelled as a single population. At the other extreme we can assume each group is fully independent, and the difficulty here is that there will be less data available and therefore more variance in our predictions.\n\nConsequently, we want an intermediate solution, acknowledging variation between groups, but allowing for data from one group to inform predictions about others. This is achieved by using a multi-level (or hierarchical) model structure. Such models allow partial sharing (or *pooling*) of information between groups, to the extent that the data indicate is appropriate. This approach results in reduced variance (when compared to a set of corresponding independent models), a shift towards a population mean (known as *shrinkage*), and generally an improved predictive performance.\n\nSounds great, right? So why would anyone ever not use this kind of model? In his [excellent blog](https://elevanth.org/blog/2017/08/24/multilevel-regression-as-default/), Richard McElreath makes the case that multi-level models should be our default approach. His greatest criticism of them is that they require some experience or training to specify and interpret. [His book](https://xcelab.net/rm/statistical-rethinking/) has a dedicated chapter to help with that. Of course, there are many better descriptions of multi-level modelling than you will get from me, but I personally found the examples in [Andrew Gelman and Jennifer Hill's book](http://www.stat.columbia.edu/~gelman/arm/) to be very helpful. Finally, [Michael Betancourt has written a much more comprehensive blog post on the topic](https://betanalpha.github.io/assets/case_studies/hierarchical_modeling.html), which includes a discussion on the underlying assumption of *exchangeability*.\n\nWe can create a partial pooling model by re-writing the below:\n\n$$\nxG = Inverse \\; Logit(\\alpha + \\beta \\cdot X)\n$$\n\nTo look like this:\n\n$$\nxG = Inverse \\; Logit(\\alpha_{[Player]} + \\beta_{[Player]} \\cdot X)\n$$\n\nIn this new structure, each parameter will now be a vector of length $N$ (where $N$ players are being considered). This means there will be a different co-efficient describing how $xG$ varies with distance from goal for each player. This makes sense as we would expect variation between players and we want our model to be able to describe it.\n\nIf each of these parameters had their own priors, we would essentially have specified $N$ independent models - one for each player. But there is a twist here: each of the vectors of co-efficients share a single prior.\n\n$$\n\\beta \\sim N(\\mu_{\\beta}, \\; \\sigma_{\\beta})\n$$\n\nThis will pull each of the individual co-efficients towards a shared mean, $\\mu_{\\beta}$. The variation between the players (for a given parameter) is characterised by $\\sigma_{\\beta}$. Rather than specify these ourselves, we will also estimate these as part of the model. This means that the extent of the pooling is conditional on the data, which is an extremely useful feature. However, we then need to include priors on these parameters, which are known as *hyperpriors*.\n\nNote that this process has introduced an extra layer (or level) to the model structure. This is why they are known as *multi-level* or *hierarchical* models. The term *partial pooling* is more a description of what they do.\n\n![In the Absence of Multi-Level Models](https://media.giphy.com/media/nXvirfLCf99rG/giphy.gif)\n\nWe see the greatest benefit of this approach when only limited data is available for one or more groups. If one player took very few shots during a period of data collection, then there will be a lot of uncertainty in their xG predictions ....*unless* we can make use of the data we have for the rest of the team.\n\n### What does this look like in `Stan`?\n\nThe below is a reduced `Stan` model, with just one co-efficient (concerning the distance from goal of the shot). This is not me being secretive, its just that the full model is quite large. You can simply add more parameters like a multi-variate linear regression on the log-odds scale, but remember that they will each require priors, hyperpriors, and data.\n\n```{stan, output.var = \"xG_model_reduced\", eval = FALSE}\ndata {\n\n  int <lower = 1> n_shots;\n  int <lower = 0, upper = 1> goal [n_shots];\n  \n  int <lower = 1> n_players;\n  int <lower = 1> player_id [n_shots];\n  \n  vector [n_shots] dist_goal;\n\n  real mu_mu_alpha;\n  real <lower = 0> sigma_mu_alpha;\n  real<lower = 0> rate_sigma_alpha;\n  \n  real mu_mu_beta_dist_goal;\n  real <lower = 0> sigma_mu_beta_dist_goal;\n  real<lower = 0> rate_sigma_beta_dist_goal;\n  \n}\n\nparameters {\n  \n  vector [n_players] alpha;\n  \n  vector [n_players] beta_dist_goal;\n\n  real mu_alpha;\n  real <lower = 0> sigma_alpha;\n  \n  real mu_beta_dist_goal;\n  real <lower = 0> sigma_beta_dist_goal;\n  \n}\n\nmodel {\n\n  // Logistic model \n  \n  goal ~ bernoulli_logit(alpha[player_id] + beta_dist_goal[player_id] .* dist_goal); \n\n  // Priors \n  \n  alpha ~ normal(mu_alpha, sigma_alpha);\n  beta_dist_goal ~ normal(mu_beta_dist_goal, sigma_beta_dist_goal);\n\n  // Hyperpriors\n  \n  mu_alpha ~ normal(mu_mu_alpha, sigma_mu_alpha);\n  sigma_alpha ~ exponential(rate_sigma_alpha);\n  \n  mu_beta_dist_goal ~ normal(mu_mu_beta_dist_goal, sigma_mu_beta_dist_goal);\n  sigma_beta_dist_goal ~ exponential(rate_sigma_beta_dist_goal);\n  \n}\n\ngenerated quantities {\n  \n  real alpha_pp = normal_rng(mu_alpha, sigma_alpha);\n  real beta_dist_goal_pp = normal_rng(mu_beta_dist_goal, sigma_beta_dist_goal);\n\n}\n```\n\nA few things that I'd like to note:\n\n - My input data is of length `n_shots` and my parameters are vectors of length `n_players`.\n - I've included my hyperpriors (the `mu_mu_...`, `sigma_mu...`, and `rate_sigma...` terms) as data, rather than *hard code* values into the file. This is so I can re-run the model with new hyperpriors without `Stan` needing to recompile.\n - Even though I have included the `mu...` and `sigma..` terms as priors in my comment, this is just to help describe the model structure. They are all included in the Parameters block of the model. As discussed above, they are inferred as part of the joint posterior distribution, meaning that we are estimating the extent of the pooling from the data.\n - I'm using the generated quantities to produce my population-level parameters, so that I have everything I need to put together probabilistic predictions in either `R` or `Python`.\n\n #### Model Parameters\n\nThe posterior distribution (which `Stan` has sampled from) is a joint probabilistic model of all parameters. Let's have a look at a few, specifically those corresponding to the effect of distance between the shot taker and goalkeeper. Shown below is the co-efficient for $6$ players (indexed $1 \\rightarrow 6$). We can see that the distance to the keeper is predicted to influence each player differently.\n\n```{r}\n#| echo: false\n#| warning: false\n\nggplot(data = xG_mcmc_samples %>% \n         dplyr::filter(grepl(pattern = 'keeper', x = Parameter)))+\n  geom_density(mapping = aes(x = value, y = ..density.., fill = as.factor(Chain)),\n                             alpha = 1/5)+\n  scale_linetype_manual(values = 2)+\n  scale_fill_viridis_d()+\n  facet_wrap(facets = ~ Parameter, ncol = 3, labeller = label_parsed)+\n  geom_vline(xintercept = 0, lty = 2, alpha = 0.2)+\n  xlim(-5/2, 5/2) + \n  labs(x = 'Value', y = 'Posterior Likelihood')+\n  DomDF::theme_ddf_light(base_family = \"Atkinson Hyperlegible\")\n\n```\n\nSome of the players will have taken fewer shots and therefore we will have less data to fit their player-specific parameters. The `mu_beta_dist_keeper` and `sigma_beta_dist_keeper` parameters in the above plot are the shared '*priors*' that control how the data from each of the players can be used to inform one another. The `beta_dist_keeper_pp` parameter is specified in the generated quantities block of my `Stan` model. It is correlated samples from the distribution characterised by the shared priors. This becomes the population (team) level co-efficient in my predictions.\n\nI've included some predictions for some actual shots taken that season in [part 1](https://allyourbayes.com/posts/xg_pt1/) of this article, but since this is the purpose of the model let's look at one more. \n\nHere is Robert Pirès goal from just outside the box at home to Bolton Wanderers in 2004. It was on his stronger (right) foot and he was not under pressure from any defenders. \n\n```{r}\n#| echo: false\n#| warning: false\n\ngoal_id <- 'e4e871b0-1521-4fa8-bc69-a8cbe223b1a5'\n\nlabel_df_Ars <- AFC_goals_df %>% \n  dplyr::filter(id == goal_id) %>% \n  mutate(label =  paste0(player.name, ', \\nxG = ', round(x = shot.statsbomb_xg, digits = 3))) %>% \n  select(id, location.x, location.y, label)\n\nArsenal_goals_plot <- SBpitch::create_Pitch(goaltype = 'box', JdeP = TRUE, BasicFeatures = FALSE)+\n  geom_point(data = AFC_goals_df %>% \n               dplyr::filter(id == goal_id),\n             mapping = aes(x = location.x, y = location.y),\n             shape = 21, alpha = 2/3, col = 'black', size = 4)+\n  ggrepel::geom_text_repel(data = label_df_Ars,\n                           mapping = aes(x = location.x, y = location.y, label = label),\n                           size = 3, family = 'Atkinson Hyperlegible', segment.alpha = 1/2)+\n  geom_segment(data = AFC_goals_df %>% \n               dplyr::filter(id == goal_id), \n              mapping = aes(x = location.x, y = location.y, xend = shot.end_location.x, yend = shot.end_location.y),\n              col = 'forestgreen', alpha = 2/3, size = 1, arrow = arrow(length = unit(0.2, 'cm')))+\n  coord_flip(xlim = c(70, 120))+\n  labs(title = \"   Pires goal vs. Bolton (March 2004)\",\n       caption = 'Data from StatsBomb |  @Domenic_DF   ')+\n  theme_void(base_size = 11, base_family = 'Atkinson Hyperlegible')+\n  theme(legend.position = 'top', legend.box = 'vertical')\n\nArsenal_goals_plot\ngrid::grid.raster(image = SB_logo, x = 1/10, y = 1/50, just = c('left', 'bottom'), width = unit(1/4, 'inches'))\n\n```\n\nAs labelled on the above plot, the StatsBomb model only gave Pirès a `r scales::percent((AFC_goals_df %>% dplyr::filter(id == goal_id))$shot.statsbomb_xg)` chance of scoring this chance. The below xG predictions are from the Bayesian partial pooling model, both for Robert Pirès (upper) and for the case where any Arsenal player could be shooting (lower). Also shown is the StatsBomb prediction. We see an improvement (since we know this chance was scored) when we make a player-specific prediction. \n\n```{r}\n#| echo: false\n#| warning: false\n\nRP_plot_df <- get_player_xG(i = 248, mcmc_df = RP_df, model_data_df = xG_model_data_processed)\n\nteam_plot_df <- get_team_xG(i = 248, mcmc_df = any_player_df, model_data_df = xG_model_data_processed)\n\nplot_df <- rbind(RP_plot_df, team_plot_df)\n\nggplot(data = plot_df)+\n  geom_density(mapping = aes(x = xG, y = ..density..),\n               fill = 'grey80', alpha = 0.8)+\n  geom_vline(mapping = aes(xintercept = xG_model_data_processed$shot.statsbomb_xg[248],\n                           linetype = 'StatsBomb xG'))+\n  scale_linetype_manual(values = c(2))+\n  facet_wrap(facets = ~ pred_player, ncol = 1)+\n  labs(y = 'Likelihood', x = 'AllYourBayes xG')+\n  DomDF::theme_ddf_light(base_family = \"Atkinson Hyperlegible\")\n\n```\n\nOur probabilistic predictions contain more information than point estimates, but for the purposes of a simpler comparison we can consider the mean value. The mean value of our team-level prediction is `r scales::percent(mean((team_plot_df)$xG))`, but conditional on the knowledge that Pirès was shooting, this becomes `r scales::percent(mean((RP_plot_df)$xG))`.\n\nIf Arsène Wenger could've chosen which of his players was presented with this opportunity, Robert Pirès would've been one of his top choices (though possible behind Thierry Henry). We have an intuitive understanding that such players have the necessary attributes to score from relatively difficult opportunities such as this, and this is accounted for in our model. We have tackled the challenge of greatly reduced (player-specific) datasets, by allowing them to share information on the basis of how similar they are. \n\n### Challenges\n\nMulti-level models capture the multi-level structure of hierarchical (nested) datasets, accounting for both variability and commonality between different groups (in this example: between different players in a team). However, as we can see from the previous plot, by introducing a set of parameters for each group and relating them all in this way, the posterior distribution now has many more dimensions and is more challenging to sample from. If you are using `Stan` you may now see more warning messages regarding *divergent transitions* - a concept that José Mourinho is acting out, below. If you do run into these problems, I would recommend reviewing the [guidance in the Stan manual on reparameterisation](https://mc-stan.org/docs/2_25/stan-users-guide/reparameterization-section.html) (writing your same model on a new scale, such that it is easier for the software to work with).\n\n![Mou's Divergent Transitions](https://media.giphy.com/media/140EFtM0NCyjHq/giphy.gif)\n\nFinally, I have published a paper demonstrating this modelling approach in an engineering context, which includes additional details for anyone who is interested: ['Consistent and coherent treatment of uncertainties and dependencies in fatigue crack growth calculations using multi-level Bayesian models'](https://www.sciencedirect.com/science/article/abs/pii/S0951832020306189?via=ihub).","srcMarkdownNoYaml":"\n\n```{r}\n#| echo: false\n#| warning: false\n#| message: false\n\nknitr::opts_chunk$set(collapse = TRUE)\n```\n\n```{r}\n#| echo: false\n#| message: false\n#| warning: false\n\nlibrary(extrafont); library(tidyverse); library(DomDF)\n\nlibrary(magick)\nSB_logo <- magick::image_read(path = \"SB_logo.png\")\n\nlibrary(StatsBombR)\n\nget_player_xG <- function(i, mcmc_df, model_data_df){\n\n  log_odds <- (mcmc_df$alpha + \n                 mcmc_df$beta_dist_goal * model_data_df$DistToGoal[i] +\n                 mcmc_df$beta_dist_keeper * model_data_df$DistToKeeper[i] +\n                 mcmc_df$beta_angle_dev * model_data_df$AngleDeviation[i] +\n                 mcmc_df$beta_angle_goal * model_data_df$AngleToGoal[i] +\n                 mcmc_df$beta_cone_def * model_data_df$DefendersInCone[i] +\n                 mcmc_df$beta_pressure * model_data_df$under_pressure[i] +\n                 mcmc_df$beta_with_head * model_data_df$shot.body_part.name_processed_Head[i] +\n                 mcmc_df$beta_with_weak_foot * model_data_df$shot.body_part.name_processed_Weaker.Foot[i])\n  \n  player <- model_data_df$player.name[i]; pred_player <- mcmc_df$player[i]\n\n  results_df <- data.frame(xG = (exp(x = log_odds) / (1 + exp(x = log_odds))), \n                           player = player,\n                           pred_player = pred_player) %>% \n    as_tibble()\n  \n  return(results_df)\n  \n}\n\nget_team_xG <- function(i, mcmc_df, model_data_df, n_prob = 1e3){\n  \n  log_odds <- (mcmc_df$alpha_pp + \n                 mcmc_df$beta_dist_goal_pp * model_data_df$DistToGoal[i] +\n                 mcmc_df$beta_dist_keeper_pp * model_data_df$DistToKeeper[i] +\n                 mcmc_df$beta_angle_dev_pp * model_data_df$AngleDeviation[i] +\n                 mcmc_df$beta_angle_goal_pp * model_data_df$AngleToGoal[i] +\n                 mcmc_df$beta_cone_def_pp * model_data_df$DefendersInCone[i] +\n                 mcmc_df$beta_pressure_pp * model_data_df$under_pressure[i] +\n                 mcmc_df$beta_with_head_pp * model_data_df$shot.body_part.name_processed_Head[i] +\n                 mcmc_df$beta_with_weak_foot_pp * model_data_df$shot.body_part.name_processed_Weaker.Foot[i])\n  \n  player <- model_data_df$player.name[i]\n  \n  results_df <- data.frame(xG = (exp(x = log_odds) / (1 + exp(x = log_odds))), \n                           player = player,\n                           pred_player = 'Unknown Player') %>% \n    as_tibble()\n  \n  return(results_df)\n  \n}\n\nAFC_goals_df <- read_csv(file = 'AFC_goals_df.csv', col_names = T, col_select = -c(...1))\n\nAFC_df <- read_csv(file = 'xG_model_data.csv', col_names = T, col_select = -c(...1))\n\nxG_mcmc_samples <- read_csv(file = 'xG_mcmc_samples.csv') |> select(c(Iteration, Chain, Parameter, value))\n\nxG_model_data_processed <- read_csv(file = 'model_data_processed.csv', col_names = T, col_select = -c(...1))\n\nany_player_df <- xG_mcmc_samples %>% \n  dplyr::filter(grepl(pattern = '_pp', x = Parameter)) %>% \n  tidyr::pivot_wider(names_from = Parameter, values_from = value)\n\nRP_df <- xG_mcmc_samples %>% \n  dplyr::filter(grepl(pattern = '[5]', x = Parameter)) %>% \n  mutate(Parameter = str_remove(string = Parameter, pattern = '.5]')) %>% \n  tidyr::pivot_wider(names_from = Parameter, values_from = value) %>% \n  mutate(player = unique(xG_model_data_processed$player.name)[5])\n\n```\n\n### TLDR\n\nThis is part 2 of an article on fitting a Bayesian partial pooling model to predict expected goals. It has the benefits of (a) quantifying *aleatory and epistemic* uncertainty, and (b) making both group-level (player-specific) and population-level (team-specific) probabilistic predictions. If you are interested in these ideas but not in statistical language, then you can also check out [part 1](https://allyourbayes.com/posts/xg_pt1/).\n\n---\n\n### Expected Goals\n\nExpected Goals (or *xG*) is a metric that was developed to predict the probability of a football (soccer) player scoring a goal, conditional on some mathematical characterisation of the shooting opportunity. Since we have a binary outcome (he or she will either score or not score) we can use everyone's favourite GLM - logistic regression.\n\nUnfortunately this causes some overlap with a [previous blog post - '*Bayesian Logistic Regression with Stan*'](https://www.allyourbayes.com/post/2020-02-14-bayesian-logistic-regression-with-stan/), but don't worry - the focus here is all about *Partial Pooling*.\n\nFirst let's look at a non-Bayesian base case. [StatsBomb](https://statsbomb.com/) have kindly made lots of football data freely available in [their R package](https://github.com/statsbomb/StatsBombR). The below creates a dataframe of the shots taken by Arsenal FC during the `2003`-`04` Premier League winning season.\n\n```{r}\n#| echo: true\n#| eval: false\n#| warning: false\nlibrary(StatsBombR); library(tidyverse)\n\nPrem_SB_matches <- FreeMatches(Competitions = SB_comps %>% \n                               dplyr::filter(competition_name == 'Premier League') %>% \n                               dplyr::filter(competition_gender == 'male'))\n\nArsenal_0304_shots <- StatsBombFreeEvents(MatchesDF = Prem_SB_matches, \n                                          Parallel = TRUE) %>% \n  allclean() %>% \n  dplyr::filter(type.name == 'Shot') %>% \n  dplyr::filter(possession_team.name == 'Arsenal')\n\n```\n\nUsing `R`'s `tidymodels` framework - make sure to have a look at [Julia Silge's tutorials](https://www.youtube.com/channel/UCTTBgWyJl2HrrhQOOc710kA) if you are unfamiliar - we can specify and fit a logistic regression. The below compares our results (including confidence intervals) to those from StatsBomb. \n\nIf you are interested in creating something similar yourself, this model has standardised inputs for parameters with relatively large values (such as angles and distances) and one hot encoding of categorical inputs (such as whether or not the shot was taken with a players weaker foot).\n\n```{r}\n#| echo: false\n#| warning: false\n\npred_df <- read_csv('pred_df.csv', col_names = T, col_select = -c(...1)) %>%\n  mutate(goal_desc = case_when(\n    goal == 0 ~ 'Shot Missed',\n    goal == 1 ~ 'Shot Scored'\n  ))\n\nggplot(data = pred_df)+\n  geom_point(mapping = aes(x = .pred_1, y = shot.statsbomb_xg),\n             size = 3, alpha = 0.4)+\n  geom_errorbar(mapping = aes(y = shot.statsbomb_xg,\n                              xmin = .pred_lower_1, xmax = .pred_upper_1),\n                width = 0.01, alpha = 0.2)+\n  facet_wrap(facets = ~ goal_desc)+\n  labs(y = 'StatsBomb xG', x = 'tidymodels xG')+\n  geom_vline(xintercept = 0.5, col = 'firebrick', alpha = 0.4, lty = 2)+\n  geom_hline(yintercept = 0.5, col = 'firebrick', alpha = 0.4, lty = 2)+\n  DomDF::theme_ddf_light(base_family = \"Atkinson Hyperlegible\")\n\n```\n\nSince we have used StatsBomb data (though their model will no doubt be based on a much larger collection) we would expect our results to be similar to theirs, and they are. Considering just the point estimates, the two models appear to broadly agree, especially when both are predicting a very low or a very high xG.\n\nHowever, some of the confidence intervals on our `tidymodels` predictions are very large. Although we would generally expect these to decrease as we introduced more data, we know that football matches (and especially specific events within football matches) are full of uncertainty. If we want to be able to quantify this uncertainty in a more useful way (we do) - we want a Bayesian model. The below section details the specific type of Bayesian model that I'm proposing for estimating xG.\n\n### Multi-Level (Partial Pooling) Models\n\nHierarchical (or 'nested') data contains multiple groups within a population, such as players with a football team. Unfortunately, this information is lost (and bias is introduced) when such data is modelled as a single population. At the other extreme we can assume each group is fully independent, and the difficulty here is that there will be less data available and therefore more variance in our predictions.\n\nConsequently, we want an intermediate solution, acknowledging variation between groups, but allowing for data from one group to inform predictions about others. This is achieved by using a multi-level (or hierarchical) model structure. Such models allow partial sharing (or *pooling*) of information between groups, to the extent that the data indicate is appropriate. This approach results in reduced variance (when compared to a set of corresponding independent models), a shift towards a population mean (known as *shrinkage*), and generally an improved predictive performance.\n\nSounds great, right? So why would anyone ever not use this kind of model? In his [excellent blog](https://elevanth.org/blog/2017/08/24/multilevel-regression-as-default/), Richard McElreath makes the case that multi-level models should be our default approach. His greatest criticism of them is that they require some experience or training to specify and interpret. [His book](https://xcelab.net/rm/statistical-rethinking/) has a dedicated chapter to help with that. Of course, there are many better descriptions of multi-level modelling than you will get from me, but I personally found the examples in [Andrew Gelman and Jennifer Hill's book](http://www.stat.columbia.edu/~gelman/arm/) to be very helpful. Finally, [Michael Betancourt has written a much more comprehensive blog post on the topic](https://betanalpha.github.io/assets/case_studies/hierarchical_modeling.html), which includes a discussion on the underlying assumption of *exchangeability*.\n\nWe can create a partial pooling model by re-writing the below:\n\n$$\nxG = Inverse \\; Logit(\\alpha + \\beta \\cdot X)\n$$\n\nTo look like this:\n\n$$\nxG = Inverse \\; Logit(\\alpha_{[Player]} + \\beta_{[Player]} \\cdot X)\n$$\n\nIn this new structure, each parameter will now be a vector of length $N$ (where $N$ players are being considered). This means there will be a different co-efficient describing how $xG$ varies with distance from goal for each player. This makes sense as we would expect variation between players and we want our model to be able to describe it.\n\nIf each of these parameters had their own priors, we would essentially have specified $N$ independent models - one for each player. But there is a twist here: each of the vectors of co-efficients share a single prior.\n\n$$\n\\beta \\sim N(\\mu_{\\beta}, \\; \\sigma_{\\beta})\n$$\n\nThis will pull each of the individual co-efficients towards a shared mean, $\\mu_{\\beta}$. The variation between the players (for a given parameter) is characterised by $\\sigma_{\\beta}$. Rather than specify these ourselves, we will also estimate these as part of the model. This means that the extent of the pooling is conditional on the data, which is an extremely useful feature. However, we then need to include priors on these parameters, which are known as *hyperpriors*.\n\nNote that this process has introduced an extra layer (or level) to the model structure. This is why they are known as *multi-level* or *hierarchical* models. The term *partial pooling* is more a description of what they do.\n\n![In the Absence of Multi-Level Models](https://media.giphy.com/media/nXvirfLCf99rG/giphy.gif)\n\nWe see the greatest benefit of this approach when only limited data is available for one or more groups. If one player took very few shots during a period of data collection, then there will be a lot of uncertainty in their xG predictions ....*unless* we can make use of the data we have for the rest of the team.\n\n### What does this look like in `Stan`?\n\nThe below is a reduced `Stan` model, with just one co-efficient (concerning the distance from goal of the shot). This is not me being secretive, its just that the full model is quite large. You can simply add more parameters like a multi-variate linear regression on the log-odds scale, but remember that they will each require priors, hyperpriors, and data.\n\n```{stan, output.var = \"xG_model_reduced\", eval = FALSE}\ndata {\n\n  int <lower = 1> n_shots;\n  int <lower = 0, upper = 1> goal [n_shots];\n  \n  int <lower = 1> n_players;\n  int <lower = 1> player_id [n_shots];\n  \n  vector [n_shots] dist_goal;\n\n  real mu_mu_alpha;\n  real <lower = 0> sigma_mu_alpha;\n  real<lower = 0> rate_sigma_alpha;\n  \n  real mu_mu_beta_dist_goal;\n  real <lower = 0> sigma_mu_beta_dist_goal;\n  real<lower = 0> rate_sigma_beta_dist_goal;\n  \n}\n\nparameters {\n  \n  vector [n_players] alpha;\n  \n  vector [n_players] beta_dist_goal;\n\n  real mu_alpha;\n  real <lower = 0> sigma_alpha;\n  \n  real mu_beta_dist_goal;\n  real <lower = 0> sigma_beta_dist_goal;\n  \n}\n\nmodel {\n\n  // Logistic model \n  \n  goal ~ bernoulli_logit(alpha[player_id] + beta_dist_goal[player_id] .* dist_goal); \n\n  // Priors \n  \n  alpha ~ normal(mu_alpha, sigma_alpha);\n  beta_dist_goal ~ normal(mu_beta_dist_goal, sigma_beta_dist_goal);\n\n  // Hyperpriors\n  \n  mu_alpha ~ normal(mu_mu_alpha, sigma_mu_alpha);\n  sigma_alpha ~ exponential(rate_sigma_alpha);\n  \n  mu_beta_dist_goal ~ normal(mu_mu_beta_dist_goal, sigma_mu_beta_dist_goal);\n  sigma_beta_dist_goal ~ exponential(rate_sigma_beta_dist_goal);\n  \n}\n\ngenerated quantities {\n  \n  real alpha_pp = normal_rng(mu_alpha, sigma_alpha);\n  real beta_dist_goal_pp = normal_rng(mu_beta_dist_goal, sigma_beta_dist_goal);\n\n}\n```\n\nA few things that I'd like to note:\n\n - My input data is of length `n_shots` and my parameters are vectors of length `n_players`.\n - I've included my hyperpriors (the `mu_mu_...`, `sigma_mu...`, and `rate_sigma...` terms) as data, rather than *hard code* values into the file. This is so I can re-run the model with new hyperpriors without `Stan` needing to recompile.\n - Even though I have included the `mu...` and `sigma..` terms as priors in my comment, this is just to help describe the model structure. They are all included in the Parameters block of the model. As discussed above, they are inferred as part of the joint posterior distribution, meaning that we are estimating the extent of the pooling from the data.\n - I'm using the generated quantities to produce my population-level parameters, so that I have everything I need to put together probabilistic predictions in either `R` or `Python`.\n\n #### Model Parameters\n\nThe posterior distribution (which `Stan` has sampled from) is a joint probabilistic model of all parameters. Let's have a look at a few, specifically those corresponding to the effect of distance between the shot taker and goalkeeper. Shown below is the co-efficient for $6$ players (indexed $1 \\rightarrow 6$). We can see that the distance to the keeper is predicted to influence each player differently.\n\n```{r}\n#| echo: false\n#| warning: false\n\nggplot(data = xG_mcmc_samples %>% \n         dplyr::filter(grepl(pattern = 'keeper', x = Parameter)))+\n  geom_density(mapping = aes(x = value, y = ..density.., fill = as.factor(Chain)),\n                             alpha = 1/5)+\n  scale_linetype_manual(values = 2)+\n  scale_fill_viridis_d()+\n  facet_wrap(facets = ~ Parameter, ncol = 3, labeller = label_parsed)+\n  geom_vline(xintercept = 0, lty = 2, alpha = 0.2)+\n  xlim(-5/2, 5/2) + \n  labs(x = 'Value', y = 'Posterior Likelihood')+\n  DomDF::theme_ddf_light(base_family = \"Atkinson Hyperlegible\")\n\n```\n\nSome of the players will have taken fewer shots and therefore we will have less data to fit their player-specific parameters. The `mu_beta_dist_keeper` and `sigma_beta_dist_keeper` parameters in the above plot are the shared '*priors*' that control how the data from each of the players can be used to inform one another. The `beta_dist_keeper_pp` parameter is specified in the generated quantities block of my `Stan` model. It is correlated samples from the distribution characterised by the shared priors. This becomes the population (team) level co-efficient in my predictions.\n\nI've included some predictions for some actual shots taken that season in [part 1](https://allyourbayes.com/posts/xg_pt1/) of this article, but since this is the purpose of the model let's look at one more. \n\nHere is Robert Pirès goal from just outside the box at home to Bolton Wanderers in 2004. It was on his stronger (right) foot and he was not under pressure from any defenders. \n\n```{r}\n#| echo: false\n#| warning: false\n\ngoal_id <- 'e4e871b0-1521-4fa8-bc69-a8cbe223b1a5'\n\nlabel_df_Ars <- AFC_goals_df %>% \n  dplyr::filter(id == goal_id) %>% \n  mutate(label =  paste0(player.name, ', \\nxG = ', round(x = shot.statsbomb_xg, digits = 3))) %>% \n  select(id, location.x, location.y, label)\n\nArsenal_goals_plot <- SBpitch::create_Pitch(goaltype = 'box', JdeP = TRUE, BasicFeatures = FALSE)+\n  geom_point(data = AFC_goals_df %>% \n               dplyr::filter(id == goal_id),\n             mapping = aes(x = location.x, y = location.y),\n             shape = 21, alpha = 2/3, col = 'black', size = 4)+\n  ggrepel::geom_text_repel(data = label_df_Ars,\n                           mapping = aes(x = location.x, y = location.y, label = label),\n                           size = 3, family = 'Atkinson Hyperlegible', segment.alpha = 1/2)+\n  geom_segment(data = AFC_goals_df %>% \n               dplyr::filter(id == goal_id), \n              mapping = aes(x = location.x, y = location.y, xend = shot.end_location.x, yend = shot.end_location.y),\n              col = 'forestgreen', alpha = 2/3, size = 1, arrow = arrow(length = unit(0.2, 'cm')))+\n  coord_flip(xlim = c(70, 120))+\n  labs(title = \"   Pires goal vs. Bolton (March 2004)\",\n       caption = 'Data from StatsBomb |  @Domenic_DF   ')+\n  theme_void(base_size = 11, base_family = 'Atkinson Hyperlegible')+\n  theme(legend.position = 'top', legend.box = 'vertical')\n\nArsenal_goals_plot\ngrid::grid.raster(image = SB_logo, x = 1/10, y = 1/50, just = c('left', 'bottom'), width = unit(1/4, 'inches'))\n\n```\n\nAs labelled on the above plot, the StatsBomb model only gave Pirès a `r scales::percent((AFC_goals_df %>% dplyr::filter(id == goal_id))$shot.statsbomb_xg)` chance of scoring this chance. The below xG predictions are from the Bayesian partial pooling model, both for Robert Pirès (upper) and for the case where any Arsenal player could be shooting (lower). Also shown is the StatsBomb prediction. We see an improvement (since we know this chance was scored) when we make a player-specific prediction. \n\n```{r}\n#| echo: false\n#| warning: false\n\nRP_plot_df <- get_player_xG(i = 248, mcmc_df = RP_df, model_data_df = xG_model_data_processed)\n\nteam_plot_df <- get_team_xG(i = 248, mcmc_df = any_player_df, model_data_df = xG_model_data_processed)\n\nplot_df <- rbind(RP_plot_df, team_plot_df)\n\nggplot(data = plot_df)+\n  geom_density(mapping = aes(x = xG, y = ..density..),\n               fill = 'grey80', alpha = 0.8)+\n  geom_vline(mapping = aes(xintercept = xG_model_data_processed$shot.statsbomb_xg[248],\n                           linetype = 'StatsBomb xG'))+\n  scale_linetype_manual(values = c(2))+\n  facet_wrap(facets = ~ pred_player, ncol = 1)+\n  labs(y = 'Likelihood', x = 'AllYourBayes xG')+\n  DomDF::theme_ddf_light(base_family = \"Atkinson Hyperlegible\")\n\n```\n\nOur probabilistic predictions contain more information than point estimates, but for the purposes of a simpler comparison we can consider the mean value. The mean value of our team-level prediction is `r scales::percent(mean((team_plot_df)$xG))`, but conditional on the knowledge that Pirès was shooting, this becomes `r scales::percent(mean((RP_plot_df)$xG))`.\n\nIf Arsène Wenger could've chosen which of his players was presented with this opportunity, Robert Pirès would've been one of his top choices (though possible behind Thierry Henry). We have an intuitive understanding that such players have the necessary attributes to score from relatively difficult opportunities such as this, and this is accounted for in our model. We have tackled the challenge of greatly reduced (player-specific) datasets, by allowing them to share information on the basis of how similar they are. \n\n### Challenges\n\nMulti-level models capture the multi-level structure of hierarchical (nested) datasets, accounting for both variability and commonality between different groups (in this example: between different players in a team). However, as we can see from the previous plot, by introducing a set of parameters for each group and relating them all in this way, the posterior distribution now has many more dimensions and is more challenging to sample from. If you are using `Stan` you may now see more warning messages regarding *divergent transitions* - a concept that José Mourinho is acting out, below. If you do run into these problems, I would recommend reviewing the [guidance in the Stan manual on reparameterisation](https://mc-stan.org/docs/2_25/stan-users-guide/reparameterization-section.html) (writing your same model on a new scale, such that it is easier for the software to work with).\n\n![Mou's Divergent Transitions](https://media.giphy.com/media/140EFtM0NCyjHq/giphy.gif)\n\nFinally, I have published a paper demonstrating this modelling approach in an engineering context, which includes additional details for anyone who is interested: ['Consistent and coherent treatment of uncertainties and dependencies in fatigue crack growth calculations using multi-level Bayesian models'](https://www.sciencedirect.com/science/article/abs/pii/S0951832020306189?via=ihub)."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.30","theme":{"light":"flatly","dark":"darkly"},"title-block-banner":true,"title":"Uncertainty in xG. Part 2","subtitle":"Partially (Optimally) Pooling Hierarchical Data","author":"Domenic Di Francesco","date":"2021-01-07","categories":["football","analysis","xg","Stan","uncertainty","Bayes","multi-level modelling","partial pooling"],"image":"xg2.png","citation":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}