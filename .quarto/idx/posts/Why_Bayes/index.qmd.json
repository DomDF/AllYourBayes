{"title":"Why be Bayesian?","markdown":{"yaml":{"title":"Why be Bayesian?","author":"Domenic Di Francesco","date":"2020-03-24","categories":["Bayes","uncertainty","decisions"],"image":"gp.png","citation":true},"headingText":"TLDR","containsRefs":false,"markdown":"\n\n\nThis post is intended to be a high-level discussion of the merits and challenges of applied Bayesian statistics. It is intended to help the reader answer: *Is it worth me learning Bayesian statistics?* or *Should I look into using Bayesian statistics in my project?* No maths or code in this post. \n\n---\n\n#### Bayes\n![Bayes](https://media.giphy.com/media/TJBbXQooivUNq/giphy.gif)\n\n### Introduction\n\nFirstly, Bayesian...\n\n * Statistics\n * Inference\n * Modelling\n * Updating\n * Data Analysis\n  \n...can boradly be considered the same thing (certainly for the purposes of this post): **the application of Bayes theorem to quantify uncertainty**. \n\nDepending on your background, you may have preconceptions about Bayesian methods being specialist or complicated. However, analysts and researchers in many domains are increasingly experimenting with the growing catalogue of open source software and resources that is making probabilistic programming more accessible.\n\n### What does a Bayesian approach provide?\n\nBayesian statistics is not the only way to account for uncertainty in calculations. The below points describe what a Bayesian approach offers, that others don't. Note that I am only really discussing methods involving probability here, though [alternative approaches are available](https://www.springer.com/gp/book/9783540402947). \n\n#### Intuitive Interpretation of Results\n\nBayesian methods give you distributions. Your parameters (and so also your predictions) are all described as distributions. A single, joint distribution in fact, which is aligned with the evidence that you provide your model. This allows you to propagate all the uncertainties and inter-dependencies when making predictions for some new input data. By comparison, alternative (frequentist) methods typically describes uncertainty in predictions using confidence intervals, which are widely used but easy to misinterpret.\n\nConfidence intervals are calculated so that they will contain the *true* value of whatever you are trying to predict with some desired frequency. They provide no information (in the absence of additional assumptions) on how credible various possible results are. The Bayesian equivalent (sometimes called credible intervals) can be drawn anywhere on a predictive distribution. In [Pratt, Raiffa and Schlaiffer's textbook](https://mitpress.mit.edu/books/introduction-statistical-decision-theory) an example is used to highlight this difference:\n\n*Imagine the plight of the manager who exclaims, 'I understand [does he?] the meaning that the demand for XYZ will lie in the interval 973 to 1374 with confidence .90. However, I am particularly interested in the interval 1300 to 1500. What confidence can I place on that interval?'*\n*Unfortunately, this question cannot be answered. Of course, however, it is possible to give a posterior probability to that particular interval - or any other - based on the sample data and on a codification of the manager's prior judgements.*\n\nThis is a nice example (aside from assuming the manager is a male) of a simple question that we need a Bayesian posterior distribution to answer. A more succinct description of the same view from [Dan Ovando's fishery statistics blog](https://www.weirdfishes.blog/):\n\n*Bayesian credible intervals mean what we’d like Frequentist confidence intervals to mean.*\n\n#### Seamless Integration with Decision Analysis\n\nFollowing on from the previous point, an analysis that directly describes the probability of any outcome is fully compatible with a decision analysis. After completing a Bayesian analysis, identifying the optimal strategy implied by your model becomes simpler and more understandable.\n\nAs stated in [James Berger's (quite theoretical) book on Bayesian statistics](https://www.springer.com/gp/book/9780387960982):\n\n*Bayesian analysis and decision theory go rather naturally together, partly because of their common goal of utilizing non-experimental sources of information, and partly because of deep theoretical ties.*\n\n#### Flexible Modelling\n\nSo this one is based on a point made in [Ben Lambert's book on Bayesian statistics](https://uk.sagepub.com/en-gb/eur/a-student%E2%80%99s-guide-to-bayesian-statistics/book245409). It is regarding how modern Bayesian statistics is achieved in practice. The computational methods may require some effort to pick up, especially if you do not have experience with programming (though Ben's book gives a nice introduction to [Stan](https://mc-stan.org/)). However, they can be readily extended to larger and more complex models. \n\nEssentially, we can propose weird and bespoke model structures, and then use the same approach to fit them. \n\n![Some Compelling Arguments](https://media.giphy.com/media/WiyczarN2XMm4/giphy.gif)\n\n### Challenges & Difficulties\n\nSo why would anyone ever *not* use Bayesian models when making predictions?\n\n#### Subjectivity\n\nPerhaps the most common criticism of Bayesian statistics is the requirement for priors - i.e. a starting point for your model. This initial estimate of uncertainty is a term in Bayes' theorem - but how can you estimate the extent of variability before you see it in your data? This will surely be completely subjective, so the results will vary depending on who is doing the analysis. This, understandably, doesn't seem right with a lot of casual enquirers.\n\nA common response to this accusation is that subjectivity is not an exclusive feature of Bayesian analysis (how about the whole approach you are taking to solving your problem !) *...but* at least Bayesians are required to be explicit about it. Priors mean that any subjective inputs have no-where to hide (in the code or the reporting) and so they are open to criticism. This point is discussed in **much** more detail in this paper from [Colombia University](http://www.stat.columbia.edu/~gelman/research/published/gelman_hennig_full_discussion.pdf).\n\nPriors can contain, as much or as little, information as desired. However, even in instances where you may feel you don't have any upfront knowledge of a problem, they represent a valuable opportunity for introducing regularisation (which protects against bad predictions due to overfitting). This idea is discussed in detail in [Richard McElreath's textbook](https://www.crcpress.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919). I would suggest that in plenty of cases, we often have a fair idea of what is happening even before we collect and analyse data, and starting from zero everytime is just not sensible. Will a doctor who has just evaluated you base their diagnosis entirely on a test outcome, or will they incorporate that eveidence into their existing understanding of your circumstances and symptoms? Bayesian statistics is the means by which this can be done mathematically.\n\n#### Computational Requirements\n\nIn practice, statisticians estimate Bayesian posterior distributions using very clever Markov Chain Monte Carlo (MCMC) sampling algorithms, that are run by their favourite probabilistic programming software. The models that I have worked with during my PhD have taken several hours to finish sampling from, but I have met statisticians whose models run for days or even weeks. Following this, there are checks that need to be completed as there are plenty of things that can go wrong with MCMC. There is a nice discussion of a recommended Bayesian workflow to make sure you are checking what you need to [here](https://arxiv.org/abs/2011.01808).\n\nMy background is in mechanical and civil engineering. In discussions with engineering researchers at conferences I have often been told that the errors and complications they encountered when playing around with Bayesian software caused them to abandon the approach in favour of more established, less informative analysis. These are challenges that I imagine everyone who has attempted modern Bayesian statistics will have encountered and resolving them can require a deep understanding of your model, and perhaps some formal training. In addition some programming *tricks* like reparameterisation (describing your model in a seemingly equivalent way, but one that is much friendlier to your software) can also help.  \n\n### Conclusions\n\nRegardless of whether you believe we exist in a deterministic universe or not, you will never have perfect state of knowledge describing your problem: uncertainty exists, so we need a sensible and safe way of accounting for it. It's difficult to escape the implications of the Maths - for instance, in avoiding quantifying uncertainty, we are often making some implicit assumptions (the implications of which can be difficult to justify) that we may not want to propagate into our decision-making. There is generally a trade-off for apparent conveniences.\n\nI believe that Bayesian statistics is actually well suited to traditional engineering problems, which are concerned with managing risk when confronted with small, messy datasets and models with plenty of uncertainty. As suggested in the earlier description of confidence intervals, frequentist statistics defines probability based on occurrences of events following a large number of trials or samples. When trying to understand the behaviour of small datasets (or even unique structural systems), Bayesian statistics can shine by comparison. \n\nVery large datasets may contain enough information to precisely estimate parameters in a model using conventional machine learning methods, and so it becomes less worthwhile running simulations to characterise variability. But how common are these big data problems in science and engineering? Sometimes large populations of data are better described as multiple smaller constituent groups, after accounting for key differences between them. Bayesian statistics has a very useful way of managing such problems by structuring models hierarchically. This method allows for **partial pooling of information** between groups, so that predictions account for the variability and commonality between groups. I will provide a detailed example of this in a future post.\n\nBayesian inference requires (computational and personal) effort to apply. But it provides results that are generally more interpretable and closely related to the actual questions we want to answer. Whether or not these methods are worth learning will of course depend on personal circumstances. I encountered this field during my PhD, and so had plenty of time to read and play with them, which I appreicate is a privelidged position to be in. \n\n![Boring, isn't it? Writing, Fitting and Evaluating Bayesian Models All Day....](https://media.giphy.com/media/WPLPEu0GUp41W/giphy.gif)","srcMarkdownNoYaml":"\n\n### TLDR\n\nThis post is intended to be a high-level discussion of the merits and challenges of applied Bayesian statistics. It is intended to help the reader answer: *Is it worth me learning Bayesian statistics?* or *Should I look into using Bayesian statistics in my project?* No maths or code in this post. \n\n---\n\n#### Bayes\n![Bayes](https://media.giphy.com/media/TJBbXQooivUNq/giphy.gif)\n\n### Introduction\n\nFirstly, Bayesian...\n\n * Statistics\n * Inference\n * Modelling\n * Updating\n * Data Analysis\n  \n...can boradly be considered the same thing (certainly for the purposes of this post): **the application of Bayes theorem to quantify uncertainty**. \n\nDepending on your background, you may have preconceptions about Bayesian methods being specialist or complicated. However, analysts and researchers in many domains are increasingly experimenting with the growing catalogue of open source software and resources that is making probabilistic programming more accessible.\n\n### What does a Bayesian approach provide?\n\nBayesian statistics is not the only way to account for uncertainty in calculations. The below points describe what a Bayesian approach offers, that others don't. Note that I am only really discussing methods involving probability here, though [alternative approaches are available](https://www.springer.com/gp/book/9783540402947). \n\n#### Intuitive Interpretation of Results\n\nBayesian methods give you distributions. Your parameters (and so also your predictions) are all described as distributions. A single, joint distribution in fact, which is aligned with the evidence that you provide your model. This allows you to propagate all the uncertainties and inter-dependencies when making predictions for some new input data. By comparison, alternative (frequentist) methods typically describes uncertainty in predictions using confidence intervals, which are widely used but easy to misinterpret.\n\nConfidence intervals are calculated so that they will contain the *true* value of whatever you are trying to predict with some desired frequency. They provide no information (in the absence of additional assumptions) on how credible various possible results are. The Bayesian equivalent (sometimes called credible intervals) can be drawn anywhere on a predictive distribution. In [Pratt, Raiffa and Schlaiffer's textbook](https://mitpress.mit.edu/books/introduction-statistical-decision-theory) an example is used to highlight this difference:\n\n*Imagine the plight of the manager who exclaims, 'I understand [does he?] the meaning that the demand for XYZ will lie in the interval 973 to 1374 with confidence .90. However, I am particularly interested in the interval 1300 to 1500. What confidence can I place on that interval?'*\n*Unfortunately, this question cannot be answered. Of course, however, it is possible to give a posterior probability to that particular interval - or any other - based on the sample data and on a codification of the manager's prior judgements.*\n\nThis is a nice example (aside from assuming the manager is a male) of a simple question that we need a Bayesian posterior distribution to answer. A more succinct description of the same view from [Dan Ovando's fishery statistics blog](https://www.weirdfishes.blog/):\n\n*Bayesian credible intervals mean what we’d like Frequentist confidence intervals to mean.*\n\n#### Seamless Integration with Decision Analysis\n\nFollowing on from the previous point, an analysis that directly describes the probability of any outcome is fully compatible with a decision analysis. After completing a Bayesian analysis, identifying the optimal strategy implied by your model becomes simpler and more understandable.\n\nAs stated in [James Berger's (quite theoretical) book on Bayesian statistics](https://www.springer.com/gp/book/9780387960982):\n\n*Bayesian analysis and decision theory go rather naturally together, partly because of their common goal of utilizing non-experimental sources of information, and partly because of deep theoretical ties.*\n\n#### Flexible Modelling\n\nSo this one is based on a point made in [Ben Lambert's book on Bayesian statistics](https://uk.sagepub.com/en-gb/eur/a-student%E2%80%99s-guide-to-bayesian-statistics/book245409). It is regarding how modern Bayesian statistics is achieved in practice. The computational methods may require some effort to pick up, especially if you do not have experience with programming (though Ben's book gives a nice introduction to [Stan](https://mc-stan.org/)). However, they can be readily extended to larger and more complex models. \n\nEssentially, we can propose weird and bespoke model structures, and then use the same approach to fit them. \n\n![Some Compelling Arguments](https://media.giphy.com/media/WiyczarN2XMm4/giphy.gif)\n\n### Challenges & Difficulties\n\nSo why would anyone ever *not* use Bayesian models when making predictions?\n\n#### Subjectivity\n\nPerhaps the most common criticism of Bayesian statistics is the requirement for priors - i.e. a starting point for your model. This initial estimate of uncertainty is a term in Bayes' theorem - but how can you estimate the extent of variability before you see it in your data? This will surely be completely subjective, so the results will vary depending on who is doing the analysis. This, understandably, doesn't seem right with a lot of casual enquirers.\n\nA common response to this accusation is that subjectivity is not an exclusive feature of Bayesian analysis (how about the whole approach you are taking to solving your problem !) *...but* at least Bayesians are required to be explicit about it. Priors mean that any subjective inputs have no-where to hide (in the code or the reporting) and so they are open to criticism. This point is discussed in **much** more detail in this paper from [Colombia University](http://www.stat.columbia.edu/~gelman/research/published/gelman_hennig_full_discussion.pdf).\n\nPriors can contain, as much or as little, information as desired. However, even in instances where you may feel you don't have any upfront knowledge of a problem, they represent a valuable opportunity for introducing regularisation (which protects against bad predictions due to overfitting). This idea is discussed in detail in [Richard McElreath's textbook](https://www.crcpress.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919). I would suggest that in plenty of cases, we often have a fair idea of what is happening even before we collect and analyse data, and starting from zero everytime is just not sensible. Will a doctor who has just evaluated you base their diagnosis entirely on a test outcome, or will they incorporate that eveidence into their existing understanding of your circumstances and symptoms? Bayesian statistics is the means by which this can be done mathematically.\n\n#### Computational Requirements\n\nIn practice, statisticians estimate Bayesian posterior distributions using very clever Markov Chain Monte Carlo (MCMC) sampling algorithms, that are run by their favourite probabilistic programming software. The models that I have worked with during my PhD have taken several hours to finish sampling from, but I have met statisticians whose models run for days or even weeks. Following this, there are checks that need to be completed as there are plenty of things that can go wrong with MCMC. There is a nice discussion of a recommended Bayesian workflow to make sure you are checking what you need to [here](https://arxiv.org/abs/2011.01808).\n\nMy background is in mechanical and civil engineering. In discussions with engineering researchers at conferences I have often been told that the errors and complications they encountered when playing around with Bayesian software caused them to abandon the approach in favour of more established, less informative analysis. These are challenges that I imagine everyone who has attempted modern Bayesian statistics will have encountered and resolving them can require a deep understanding of your model, and perhaps some formal training. In addition some programming *tricks* like reparameterisation (describing your model in a seemingly equivalent way, but one that is much friendlier to your software) can also help.  \n\n### Conclusions\n\nRegardless of whether you believe we exist in a deterministic universe or not, you will never have perfect state of knowledge describing your problem: uncertainty exists, so we need a sensible and safe way of accounting for it. It's difficult to escape the implications of the Maths - for instance, in avoiding quantifying uncertainty, we are often making some implicit assumptions (the implications of which can be difficult to justify) that we may not want to propagate into our decision-making. There is generally a trade-off for apparent conveniences.\n\nI believe that Bayesian statistics is actually well suited to traditional engineering problems, which are concerned with managing risk when confronted with small, messy datasets and models with plenty of uncertainty. As suggested in the earlier description of confidence intervals, frequentist statistics defines probability based on occurrences of events following a large number of trials or samples. When trying to understand the behaviour of small datasets (or even unique structural systems), Bayesian statistics can shine by comparison. \n\nVery large datasets may contain enough information to precisely estimate parameters in a model using conventional machine learning methods, and so it becomes less worthwhile running simulations to characterise variability. But how common are these big data problems in science and engineering? Sometimes large populations of data are better described as multiple smaller constituent groups, after accounting for key differences between them. Bayesian statistics has a very useful way of managing such problems by structuring models hierarchically. This method allows for **partial pooling of information** between groups, so that predictions account for the variability and commonality between groups. I will provide a detailed example of this in a future post.\n\nBayesian inference requires (computational and personal) effort to apply. But it provides results that are generally more interpretable and closely related to the actual questions we want to answer. Whether or not these methods are worth learning will of course depend on personal circumstances. I encountered this field during my PhD, and so had plenty of time to read and play with them, which I appreicate is a privelidged position to be in. \n\n![Boring, isn't it? Writing, Fitting and Evaluating Bayesian Models All Day....](https://media.giphy.com/media/WPLPEu0GUp41W/giphy.gif)"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.30","theme":{"light":"flatly","dark":"darkly"},"title-block-banner":true,"title":"Why be Bayesian?","author":"Domenic Di Francesco","date":"2020-03-24","categories":["Bayes","uncertainty","decisions"],"image":"gp.png","citation":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}