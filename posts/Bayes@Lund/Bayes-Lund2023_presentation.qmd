---
title: "Identifying Data Requirements using Bayesian Decision Analysis"
subtitle: "Bayes@Lund2023"
citation: true
execute: 
  echo: true
  message: false
  warning: false
format: 
  revealjs:
    progress: true
    chalkboard: false
    preview-links: true
    theme: [simple, custom.scss]
    smaller: true
    scrollable: true
    transition: slide
    institute: "The Alan Turing Institute <br> CSML Research Group, Civil Engineering, University of Cambridge"
    author: "Domenic Di Francesco, PhD, CEng (MIMechE)"
    code-fold: true
    date: "23 January 2023"
    date-format: full
---

## TLDR

These are the slides I presented at the Bayes@Lund2023 conference in January. I spoke about identifying data requirements (for engineering systems) using value of information analysis.

## Uncertainty and Decisions

```{r}
#| echo: false
library(tidyverse); setwd("~/Github/Bayes-Lund2023/Bayes-Lund2023_presentation")
```

::: {.incremental}
 - *"Many if not most statistical analyses are performed for the ultimate goal of decision making."*
 <br> Andrew Gelman et. al, [Bayesian Data Analysis](https://doi.org/10.1201/b16018). <br>

 - *"Bayesian analysis and decision theory go rather naturally together, partly because of their common goal of utilizing non-experimental sources of information, and partly because of deep theoretical ties."* 
 <br> Prof. James Berger, [Statistical Decision Theory and Bayesian Analysis](https://doi.org/10.1007/978-1-4757-4286-2). <br>

 - *"It is important that all problems of inference be visualized as problems of decision."*
<br> Prof. Ian Jordaan, [Decisions Under Uncertainty](https://doi.org/10.1017/CBO9780511804861).<br>

 - *"It (statistics) is seldom really appreciated for what it can be used for, namely as a basis for assessing information and organising the process of acquiring knowledge in pursuit of supporting decision making."* 
 <br> Prof. Michael Faber, [Statistics and Probability Theory: In Pursuit of Engineering Decision Support](https://doi.org/10.1007/978-94-007-4056-3). <br>

:::

## Uncertainty and Decisions

::: panel-tabset
### Decision-event trees

![Example decision tree](Figures/decision_tree.png)

### Influence diagrams

![Example influence diagram](Figures/influence_diagram.png)

Select action $a^{*}$ associated with the highest expected utility:

```{=tex}
\begin{equation}
a^{*} = \arg \max_{a \in A} \mathop{\mathbb{E}}_{\theta \sim \pi(\theta)} \big[ u(a, \theta) \big]
\end{equation}
```

### Value of Information

![Example influence diagram](Figures/VoI.png)

::: {style="text-align: center"}

*How* and *to what extent* will this data facilitate improved decision making?

```{=tex}
\begin{equation}
VoI(e_{i}) = \mathop{\mathbb{E}}_{\theta \sim \pi(\theta), \\z \sim f(z \mid \theta)} \big[ u(e_{i}, z, a^{*}, \pi(\theta \mid z) \big] - \mathop{\mathbb{E}}_{\theta \sim \pi(\theta)} \big[ u( a^{*}, \pi(\theta)) \big]
\end{equation}
```

:::

:::

## Examples

```{julia}
# For working with data
using CSV, DataFrames, DataFramesMeta, RCall

# For describing probabilistic models
using Distributions, Random, LatinHypercubeSampling, StanSample, MCMCChains

# For describing and solving decision problem
using JuMP, HiGHS, DecisionProgramming, LinearAlgebra

```

```{julia}
#| echo: false
#| output: false

set_cmdstan_home!("/Users/ddifrancesco/.cmdstan/cmdstan-2.30.1"); CMDSTAN_HOME

```

## Building Ventilation

::: panel-tabset

### Problem

Specify the ventilation setting for an office building: 

 - Low
 - Standard
 - Well Ventilated
 - High

Higher ventilation rates decrease the risk of infection from airborne disease, but cost more to run.

[Link to paper](https://doi.org/10.1098/rspa.2020.0584)

### Influence diagram

![Influence diagram for building ventilation problem](Figures/occupancy.png)

### Model

```{julia}
#| output: false
s = 3600; venting = [1, 3, 5, 10] ./ s; κ = 0.39 / s; λ = 0.636 / s
loss_rate = venting .+ κ .+ λ; tₘ = 8 * s; n_step = 100

loss_rate = venting .+ κ .+ λ; tₘ = 8 * 3600; n_step = 100

function Pr_infection(occupancy::Int64, Vol::Float64, loss::Float64, Δt = tₘ / (n_step - 1), infection_rate = 0.02, iᵣ = 5.21 * 10^-4, Cᵣ = 410, Nᵣ = 0.453)
    C = zeros(n_step); nᵢₙₕ = zeros(n_step); Prᵢ = zeros(n_step); t = zeros(n_step); 
    N = occupancy * infection_rate
    for i in 2:(n_step)
        t[i] = t[i-1] + Δt
        C[i] = N * Nᵣ / (Vol * loss) + (C[i-1] - N * Nᵣ / (Vol * loss)) * exp(-1 * loss * Δt)
        nᵢₙₕ[i] = nᵢₙₕ[i-1] + iᵣ * Δt * C[i]
        Prᵢ[i] = 1 - exp(-1 * nᵢₙₕ[i] / Cᵣ)
    end
    return last(Prᵢ)
end

function pr_inf_pop(pr_inf::Float64, occupants::Int64)
    return Binomial(occupants, pr_inf) |> x -> pdf.(x, collect(0:occupants))
end

function draw_lhs(dist, n::Int)
    samples = randomLHC(n + 2, 1) |>
        x -> scaleLHC(x, [(0, 1)])[:, 1] |>
        x -> filter(∉((0, 1)), x) |>
        x -> sort(x) |>
        x -> quantile(dist, x)
    return samples
end

```

### Prior uncertainty

```{julia}
#| output: false
λpr = 30; n_samples = 1000
occupancy_model = Poisson(λpr); occupancy_samples = draw_lhs(occupancy_model, n_samples)

vent_options = Dict("Low" => 5, "Standard" => 30, "Well_Ventilated" => 45, "High" => 90)
vent_states = keys(vent_options) |> x -> collect(x)
vent_costs = [vent_options[state] for state in vent_states]

infection_states = ["Infected", "Uninfected"]

sickness_costs = Dict("Sick_Day" => 115 * 3)
```

```{julia}
#| echo: false
#| output: false
@rput(occupancy_samples)
```

```{r}
#| echo: false
ggplot(data = tibble(o = occupancy_samples))+
  geom_histogram(mapping = aes(x = o, y = after_stat(x = density)), 
                 binwidth = 3, col = "black", alpha = 1/3)+
  scale_x_continuous(name = "Number of Occupants")+
  scale_y_continuous(name = "Prior Likelihood")+
  ggthemes::theme_base(base_family = "Atkinson Hyperlegible", base_size = 12)+
  theme(plot.background = element_rect(colour = NA),
        legend.title = element_blank(), legend.position = "top")
```

### Decision analysis

```{julia}
function exp_opt_venting(occupants::Vector{Int64} = occupancy_samples, optimiser = HiGHS.Optimizer)

    occupancy_states = append!(["0"], string.(occupants))

    # Initialise the influence diagram
    occupancy_decision = InfluenceDiagram()

    # Create structure of influence diagram
    add_node!(occupancy_decision, DecisionNode("Ventilation", [], vent_states))
    add_node!(occupancy_decision, ChanceNode("Occupancy", [], occupancy_states))
    add_node!(occupancy_decision, ValueNode("Cost_Infection", ["Ventilation", "Occupancy"]))
    add_node!(occupancy_decision, ValueNode("Cost_Ventilation", ["Ventilation"]))

    generate_arcs!(occupancy_decision)

    # Calculate the probability of infection for each ventilation option
    pr_inf = [Pr_infection.(o, 2000.0, loss_rate) for o in occupants] |>
        x -> [v[i] for i in 1:4 for v in x] |>
        x -> [x[1:length(occupants)], 
              x[length(occupants)+1:2*length(occupants)], 
              x[2*length(occupants)+1:3*length(occupants)], 
              x[3*length(occupants)+1:4*length(occupants)]]
    pr_low = pr_inf[1]; pr_std = pr_inf[2]; pr_well = pr_inf[3]; pr_high = pr_inf[4]

    # Calculate the probability of each possible number of total infections in the building
    # ...and the associated expected costs due to sickness
    infection_df = DataFrame(occupants = occupants, 
                             pr_inf_low = pr_inf_pop.(pr_low, occupants),
                             pr_inf_std = pr_inf_pop.(pr_std, occupants),
                             pr_inf_well = pr_inf_pop.(pr_well, occupants),
                             pr_inf_high = pr_inf_pop.(pr_high, occupants)) |>
        x -> @rtransform(x, :cost_sick = [sickness_costs["Sick_Day"] * o for o in reverse(:occupants:-1:0)]) |>
        x -> @rtransform(x, :EC_low = :pr_inf_low .* :cost_sick |> x -> sum(x)) |>
        x -> @rtransform(x, :EC_std = :pr_inf_std .* :cost_sick |> x -> sum(x)) |>
        x -> @rtransform(x, :EC_well = :pr_inf_well .* :cost_sick |> x -> sum(x)) |>
        x -> @rtransform(x, :EC_high = :pr_inf_high .* :cost_sick |> x -> sum(x)) |>
        x -> @rselect(x, :occupants, :EC_low, :EC_std, :EC_well, :EC_high)

    # Assigning probabilities and utilities to diagram
    Pr_occ = ProbabilityMatrix(occupancy_decision, "Occupancy")
    C_inf = UtilityMatrix(occupancy_decision, "Cost_Infection")
    C_vent = UtilityMatrix(occupancy_decision, "Cost_Ventilation")

    Pr_occ = append!([0.0], repeat([1/length(occupants)], length(occupants)))

    C_inf["Low", :] = append!([0.0], infection_df.EC_low)
    C_inf["Standard", :] = append!([0.0], infection_df.EC_std)
    C_inf["Well_Ventilated", :] = append!([0.0], infection_df.EC_well)
    C_inf["High", :] = append!([0.0], infection_df.EC_high)

    C_vent["Low"] = vent_options["Low"]; C_vent["Standard"] = vent_options["Standard"]
    C_vent["Well_Ventilated"] = vent_options["Well_Ventilated"]; C_vent["High"] = vent_options["High"]

    add_probabilities!(occupancy_decision, "Occupancy", Pr_occ)
    add_utilities!(occupancy_decision, "Cost_Infection", C_inf)
    add_utilities!(occupancy_decision, "Cost_Ventilation", C_vent)

    # generate the full influence diagram
    generate_diagram!(occupancy_decision)

    # Define and run solver
    decision_model = JuMP.Model(optimiser); set_silent(decision_model)

    z = DecisionVariables(decision_model, occupancy_decision)
    x_s = PathCompatibilityVariables(decision_model, occupancy_decision, z)

    Exp_Cost = expected_value(decision_model, occupancy_decision, x_s)

    @objective(decision_model, Min, Exp_Cost)
    optimize!(decision_model)

    # Process results
    Z = DecisionStrategy(z)
    U_dist = UtilityDistribution(occupancy_decision, DecisionStrategy(z))

    exp_opt_decision = DataFrame(u_opt = LinearAlgebra.dot(U_dist.p, U_dist.u),
                                 a_opt = vent_states[argmax(Z.Z_d[1])])

    return exp_opt_decision
    
end

prior_ventilation_decision = exp_opt_venting()

```
:::

## Building Ventilation

#### Expected value of measuring occupancy

```{julia}
#| output: false

measure_occupancy_df = DataFrame()
for o in occupancy_samples
    append!(measure_occupancy_df, exp_opt_venting([o]))
end

prior_cost = prior_ventilation_decision.u_opt[1] 
prepost_cost = mean(measure_occupancy_df.u_opt)

VoPI = prior_cost - prepost_cost
```

```{julia}
#| echo: false
#| output: false
@rput(prior_ventilation_decision); @rput(measure_occupancy_df)
```

```{r}
#| echo: false
#| fig-cap: "Expected cost and optimal action associated with each simulation from a prospective measurement of building occupancy"

measure_occupancy_df$meas <- occupancy_samples
measure_occupancy_df$a_opt <- gsub(pattern = "Well_Ventilated", replacement = "Well \nVentilated", x = measure_occupancy_df$a_opt)

arrow_df <- tibble(xend = prior_ventilation_decision$u_opt,
                   x = measure_occupancy_df$u_opt |> mean(),
                   y = 4, yend = 4)

ggplot(data = bind_rows(measure_occupancy_df, tibble(a_opt = "High", u_opt = NA)) |>
         mutate(a_opt = factor(x = a_opt, levels = c("Low", "Standard", "Well \nVentilated", "High"))),
                        mapping = aes(x = u_opt, y = a_opt))+
  geom_jitter(shape = 21, alpha = 1/2, height = 1/3, mapping = aes(fill = meas))+
  geom_vline(mapping = aes(xintercept = prior_ventilation_decision$u_opt, 
                           lty = "Expected Cost Without Measuring Occupancy"), alpha = 1/2) +
  geom_vline(mapping = aes(xintercept = measure_occupancy_df$u_opt |> mean(), 
                           lty = "Expected Cost Measuring Occupancy"), alpha = 1/2)+
  geom_segment(data = arrow_df,
               mapping = aes(x = x, xend = xend, y = y, yend = yend, col = "EVoPI"), 
               arrow = arrow(length = unit(0.25, "cm"), ends = "first", type = "closed"))+
  scale_color_manual(values = c("midnightblue"))+
  scale_fill_viridis_c()+
  labs(y = "Expected Optimal Action", fill = "Measured Occupancy", lty = "", col = "")+
    scale_linetype_manual(values = c(2, 1))+
  scale_x_continuous(name = "Expected Cost", limits = c(0, 125), labels = scales::dollar_format(), breaks = scales::pretty_breaks())+
  ggthemes::theme_base(base_size = 14, base_family = "Atkinson Hyperlegible")+
  theme(legend.position = 'top', legend.title = element_text(size = 10), axis.text.y = element_text(angle = 90, hjust = 0.5), 
        plot.background = element_rect(colour = NA))+
  guides(linetype = guide_legend(nrow = 2), 
         fill = guide_colorbar(title.position = 'top', barwidth = 10, barheight = 1/2, order = 3))
```

## Inspecting for Corrosion

::: {.panel-tabset}

### Problem

Identify a repair plan for $10$ locations of corrosion damage:

```{=tex}
\begin{equation}
CGR = \dfrac{d_{i2} - d_{i1}}{T_{i2} - T_{i1}}
\end{equation}
```

 - $2$ inspections have been completed
 - Inspection $2$ was incomplete

 - Should the inspection team return to location $4$?
 
 [Link to paper](https://doi.org/10.1017/dce.2021.18)

### Influence diagram

![Influnce diagram for inspecting for corrosion](Figures/corrosion.png)

### Model

```{julia}
#| echo: false
#| output: false

inspection_df = CSV.read("data_files/inspection_data.csv", DataFrame)
years = unique(inspection_df.t); insps = unique(inspection_df.inspection); locations = unique(inspection_df.location)

function lnorm_params(μ::Float64, σ::Float64)
    sdlog = √(log(1 + σ^2 / μ^2))
    meanlog = log(μ) - 0.5 * sdlog^2
    return Dict("sdlog" => sdlog, "meanlog" => meanlog)
end

prior_depth = lnorm_params(10.0, 6.0)

model_text = open("data_files/corr_fp_md.stan") do file
    read(file, String)
end

model_data = Dict(
    "N" => nrow(inspection_df),
  "n_A" => unique(inspection_df.anomaly_id) |> x -> length(x),
  "n_M" => sum(inspection_df.missing),
  "ID" => inspection_df.anomaly_id,
  "depth_i1" => inspection_df |> x -> @rsubset(x, :t == years[1]) |> x -> x.depth_mm,
  "depth_i2" => inspection_df |> x -> @rsubset(x, :t == years[2]) |> x -> x.depth_mm,
  "error_i1" => inspection_df |> x -> @rsubset(x, :t == years[1]) |> x -> x.sizing_uncertainty,
  "error_i2" => inspection_df |> x -> @rsubset(x, :t == years[2]) |> x -> x.sizing_uncertainty,
  "d_years" => maximum(years) - minimum(years),
  "ex_1" => inspection_df |> x -> @rsubset(x, :t == years[1]) |> x -> x.missing,
  "ex_2" => inspection_df |> x -> @rsubset(x, :t == years[2]) |> x -> x.missing,
  "mu_mu_beta" => 1, "sigma_mu_beta" => 1, "rate_sigma_beta" => 1,
  "mu_depth_imp" => prior_depth["meanlog"],
  "sigma_depth_imp" => prior_depth["sdlog"]
)

tmpdir = pwd() * "/tmp"; model = SampleModel("corrosion_model", model_text, tmpdir)

n_chains = 4; n_warmup = 2_000; n_draws = Int(1_000 / n_chains)

rc = model |>
     x -> stan_sample(x; data = model_data, save_warmup = false, 
     num_warmups = n_warmup, num_samples = n_draws, thin = 1, delta = 0.85)

if success(rc)
    samples_df = read_samples(model, :mcmcchains) |> x -> DataFrame(x) |> x -> DataFrames.stack(x)
    diags_df = read_summary(model) |> x -> DataFrame(x)
end

depth_df = samples_df |>
    x -> @rsubset(x, occursin("depth_true", :variable)) |>
    x -> @rtransform(x, :insp = occursin("i1", :variable) ? "Inspection 1" : "Inspection 2") |>
    x -> @rtransform(x, :anomaly = split(:variable, ".") |> x -> "Anomaly " * x[2])

@rput(samples_df); @rput(depth_df)
```

```stan
functions{
  real log_norm_sigma(real norm_mu, real norm_sigma){ 
    return log(1 + (norm_sigma^2 / norm_mu^2)); 
  }
  
  real log_norm_mu(real norm_mu, real norm_sigma){
    return log(norm_mu) - 0.5 * log_norm_sigma(norm_mu, norm_sigma)^2;
  }
}

data {
  int <lower = 1> N; // Number of data points
  int <lower = 1> n_A; // Number of anomalies
  int <lower = 0> n_M; // Number of anomalies missed

  int <lower = 1> ID [N]; // Defect identifier

  vector [n_A] depth_i1; // Measured corrosion depth
  vector [n_A] depth_i2; // Measured corrosion depth
  vector <lower = 0> [n_A] error_i1; // Measurement error parameter
  vector <lower = 0> [n_A] error_i2; // Measurement error parameter

  real d_years; // Time of measurement
  
  real mu_mu_beta; // Prior corrosion growth rate parameter
  real <lower = 0> sigma_mu_beta; // Prior corrosion growth rate parameter
  real <lower = 0> rate_sigma_beta; // Prior corrosion growth rate parameter
  
  real mu_depth_imp;  // Prior on missing data
  real <lower = 0> sigma_depth_imp;  // Prior on missing data
}

parameters {
  real mu_beta;
  real <lower = 0> sigma_beta;
  
  vector <lower = 0> [n_A] depth_true_i1;
  vector <lower = 0> [n_A] delta_depth;
}

transformed parameters {
  vector [n_A] depth_true_i2 = depth_true_i1 + delta_depth;
  vector [n_A] growth; // Growth rate depth of corrosion
  
  for (i in 1:n_A){  
    growth[i] = (depth_true_i2[i] - depth_true_i1[i]) / (d_years);
  }
}

model {
  // Model
  for (n in 1:n_A) {
    if (ex_1[n] == 0){
      depth_i1[n] ~ normal(depth_true_i1[n], error_i1[n]);
    }
    
    if (ex_1[n] == 1) {
      depth_true_i1[n] ~ lognormal(mu_depth_imp, sigma_depth_imp);
    }
    
    if (ex_2[n] == 0){
      depth_i2[n] ~ normal(depth_true_i2[n], error_i2[n]);
    }
    
    if (ex_2[n] == 1) {
      depth_true_i2[n] ~ lognormal(mu_depth_imp, sigma_depth_imp);
    }
  }
  growth ~ lognormal(mu_beta, sigma_beta);

  // Priors
  target += normal_lpdf(mu_beta | mu_mu_beta, sigma_mu_beta);
  target += exponential_lpdf(sigma_beta | rate_sigma_beta);
}

generated quantities {
   real CGR_pp = lognormal_rng(mu_beta, sigma_beta);
}

```

### Prior uncertainty

```{r}
#| echo: false
#| fig-cap: "Estimated extent of corrosion damage for first 4 locations"

depth_df <- depth_df |>
  mutate(type = case_when(
    (insp == "Inspection 2" & anomaly == "Anomaly 4") ~ "Imputed",
    T ~ "Measured"
  ))

ggplot(data = depth_df |> dplyr::filter(anomaly %in% (depth_df$anomaly |> unique())[1:4]))+
  geom_density(mapping = aes(x = value, y = after_stat(x = density), alpha = type), 
               col = "black", fill = "grey")+
  facet_grid(anomaly ~ insp)+
  scale_x_continuous(name = "Corrosion depth, mm", breaks = scales::pretty_breaks())+
  scale_y_continuous(name = "Likelihood", breaks = scales::pretty_breaks())+
  ggthemes::theme_base(base_size = 14, base_family = "Atkinson Hyperlegible")+
  theme(legend.position = 'top', legend.title = element_blank(), 
        plot.background = element_rect(colour = NA))

```

:::

## Inspecting for Corrosion

#### Expected value of completing inspection

::: panel-tabset

### Prior decision analysis
```{r}
#| echo: false

VoInsp_prior <- read_csv("data_files/VoInsp_prior.csv", )

kableExtra::kable(VoInsp_prior)
```

### Preposterior decision analysis

```{r}
#| echo: false

VoInsp <- read_csv("data_files/VoInsp.csv")

arrow_df <- tibble(xend = sum(VoInsp_prior$Cost),
                   x = VoInsp$cost |> mean(),
                   y = 2e-4, yend = 2e-4)

VoI <- sum(VoInsp_prior$Cost) -mean(VoInsp$cost)

ggplot(data = VoInsp) +
  geom_histogram(mapping = aes(x = cost, y = after_stat(x = density)),
                 col = "black", fill = "grey")+
  geom_vline(mapping = aes(xintercept = mean(VoInsp$cost), lty = "Expected cost following \ninspection of Anomaly 4"), alpha = 2/3)+
  geom_vline(mapping = aes(xintercept = sum(VoInsp_prior$Cost), lty = "Expected cost without \ninspecting Anomaly 4"), alpha = 2/3)+
  geom_segment(data = arrow_df,
               mapping = aes(x = x, xend = xend, y = y, yend = yend, col = "Expected Value \nof Inspection"), 
               arrow = arrow(length = unit(0.25, "cm"), ends = "first", type = "closed"))+
  geom_text(data = arrow_df, mapping = aes(x = mean(c(x, xend)), y = y + 2e-5, label = paste("$", round(VoI, digits = 0))), family = "Atkinson Hyperlegible", size = 3.75)+
  scale_color_manual(values = "forestgreen")+
  scale_linetype_manual(values = c(2, 1))+
  scale_x_continuous(name = "Cost", labels = scales::dollar_format(), breaks = scales::pretty_breaks())+
  scale_y_continuous(name = "Likelihood")+
  ggthemes::theme_base(base_size = 14, base_family = "Atkinson Hyperlegible")+
  theme(legend.position = 'top', legend.title = element_blank(), 
        plot.background = element_rect(colour = NA))

```

:::

## Some concluding thoughts

- Explictly relate Bayesian models to underlying decisions:
    - Consistent and coherent results on an intereptable scale
    - Quantify expected value of collecting data

- Challenges:
    - Defining problem, e.g. identifying sources of value.
    - Computation/combinatorics...

- Further work...

## Thank you for your attention!

::: {style="text-align: center"}

<br>

{{< fa envelope size=2x >}}
[ddifrancesco@turing.ac.uk](ddifrancesco@turing.ac.uk)

<br>

{{< fa brands twitter size=2x >}} 
[@DomenicDF](https://twitter.com/Domenic_DF)

<br>

{{< fa brands github size=2x >}}
[@DomDF](https://github.com/DomDF)

:::
